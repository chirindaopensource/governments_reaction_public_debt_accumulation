{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFjAyu6ge5my"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README.md\n",
        "\n",
        "\n",
        "# A Cross-Country Analysis of Government Fiscal Reaction Functions\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Statsmodels](https://img.shields.io/badge/Statsmodels-150458.svg?style=flat&logo=python&logoColor=white)](https://www.statsmodels.org/stable/index.html)\n",
        "[![Linearmodels](https://img.shields.io/badge/Linearmodels-013243.svg?style=flat&logo=python&logoColor=white)](https://bashtage.github.io/linearmodels/)\n",
        "[![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=flat&logo=Matplotlib&logoColor=black)](https://matplotlib.org/)\n",
        "[![Seaborn](https://img.shields.io/badge/seaborn-%233776AB.svg?style=flat&logo=python&logoColor=white)](https://seaborn.pydata.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2507.13084-b31b1b.svg)](https://arxiv.org/abs/2507.13084)\n",
        "[![DOI](https://img.shields.io/badge/DOI-10.48550/arXiv.2507.13084-blue)](https://doi.org/10.48550/arXiv.2507.13084)\n",
        "[![Research](https://img.shields.io/badge/Research-Public%20Finance-green)](https://github.com/chirindaopensource/governments_reaction_public_debt_accumulation)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Macroeconometrics-blue)](https://github.com/chirindaopensource/governments_reaction_public_debt_accumulation)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-Panel%20Data%20Analysis-orange)](https://github.com/chirindaopensource/governments_reaction_public_debt_accumulation)\n",
        "[![Data Source](https://img.shields.io/badge/Data%20Source-IMF%2C%20WB%2C%20OECD-lightgrey)](https://data.imf.org/)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/governments_reaction_public_debt_accumulation)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/governments_reaction_public_debt_accumulation`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Do Governments React to Public Debt Accumulation? A Cross-Country Analysis\"** by:\n",
        "\n",
        "*   Paolo Canofari\n",
        "*   Alessandro Piergallini\n",
        "*   Marco Tedeschi\n",
        "\n",
        "The project provides a complete, end-to-end pipeline for estimating sovereign fiscal reaction functions from panel data. It replicates the paper's advanced econometric techniques, including diagnostics for cross-sectional dependence and slope heterogeneity, and features a from-scratch implementation of the Dynamic Common Correlated Effects Mean Group (DCCEMG) estimator. The goal is to provide a transparent, robust, and extensible tool for researchers, policymakers, and financial analysts to assess fiscal sustainability across countries.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: create_fiscal_credibility_report](#key-callable-create_fiscal_credibility_report)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Do Governments React to Public Debt Accumulation? A Cross-Country Analysis.\" The core of this repository is the iPython Notebook `governments_reaction_public_debt_accumulation_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final generation of results tables, interpretations, and visualizations.\n",
        "\n",
        "The central question in public finance is whether governments act to ensure their long-term solvency. This project provides the tools to answer this question by estimating a government's fiscal reaction function—a rule that describes how the primary budget balance responds to changes in the level of public debt. A positive and significant response is a key indicator of a sustainable, or \"Ricardian,\" fiscal policy.\n",
        "\n",
        "This codebase enables users to:\n",
        "-   Rigorously validate and clean macroeconomic panel data.\n",
        "-   Perform state-of-the-art panel data diagnostic tests.\n",
        "-   Estimate dynamic fiscal reaction functions using the advanced DCCEMG estimator, which accounts for global shocks and country-specific behaviors.\n",
        "-   Calculate and test the significance of the long-run fiscal response to debt.\n",
        "-   Systematically conduct robustness and sensitivity analyses to ensure the credibility of the findings.\n",
        "-   Replicate and extend the empirical results of the original research paper.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in modern panel data econometrics and the theory of fiscal sustainability.\n",
        "\n",
        "**The Fiscal Reaction Function:** The analysis centers on estimating a dynamic fiscal policy rule, as proposed by Bohn (1998). The core equation is:\n",
        "$$\n",
        "s_{it} = \\phi_i s_{i,t-1} + \\rho_i b_{i,t-1} + \\boldsymbol{\\beta_i' x_{it}} + \\epsilon_{it}\n",
        "$$\n",
        "where `s` is the primary surplus-to-GDP ratio, `b` is the debt-to-GDP ratio, and `x` is a vector of control variables (e.g., output gap, spending gap). The key hypothesis is that **`ρ > 0`**, which implies that governments raise their primary surplus in response to higher debt, ensuring solvency.\n",
        "\n",
        "**The Long-Run Response (LRR):** The ultimate measure of sustainability is the long-run response, which accounts for policy inertia (`φ`). It is calculated as:\n",
        "$$\n",
        "LRR = \\frac{\\rho}{1 - \\phi}\n",
        "$$\n",
        "\n",
        "**Econometric Challenges:** Estimating this relationship in a multi-country panel is challenging due to:\n",
        "1.  **Cross-Sectional Dependence (CSD):** Global shocks (e.g., financial crises, pandemics) affect all countries simultaneously, correlating the error terms `ε_it`.\n",
        "2.  **Slope Heterogeneity:** The policy rule parameters (`φ_i`, `ρ_i`) are likely different for each country.\n",
        "\n",
        "**The DCCEMG Estimator:** To address these challenges, the paper uses the Dynamic Common Correlated Effects Mean Group (DCCEMG) estimator (Chudik & Pesaran, 2015). This method augments each country's regression with cross-sectional averages of the variables, which act as proxies for the unobserved global shocks. By averaging the resulting coefficients across countries, it provides a consistent estimate of the average policy rule while respecting country-specific heterogeneity.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`governments_reaction_public_debt_accumulation_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Data Validation:** A rigorous validation module to check data schema, dimensions, and consistency.\n",
        "-   **Data Cleansing:** Systematic handling of missing values and economically-informed outlier clipping.\n",
        "-   **Panel Data Diagnostics:** A full suite of tests for slope homogeneity (F-test for poolability), cross-sectional dependence (Pesaran's CD test), and panel unit roots (CIPS test).\n",
        "-   **From-Scratch DCCEMG Estimator:** A complete and heavily commented implementation of the DCCE Mean Group algorithm.\n",
        "-   **Long-Run Effects with Delta Method:** Precise calculation of the long-run response and its standard error using the Delta Method.\n",
        "-   **Automated Robustness & Sensitivity Suite:** A framework to automatically re-run the entire pipeline under alternative specifications (e.g., different lags, GFC definitions, data winsorization, HP filter parameters).\n",
        "-   **Programmatic Reporting:** Automated generation of a publication-quality results table, a detailed textual interpretation of the findings, and key visualizations.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Data Preparation (Tasks 1-3):** The pipeline ingests raw panel data, validates its structure, cleanses it, and sets a proper `MultiIndex` for panel analysis.\n",
        "2.  **Diagnostics (Tasks 4-5):** It generates descriptive statistics (replicating Table 1) and runs the full suite of panel diagnostic tests to justify the choice of the DCCEMG estimator.\n",
        "3.  **Estimation (Task 6):** It estimates the 10 different model specifications from Table 2 using the from-scratch DCCEMG estimator, paying strict attention to the correct, subsample-specific calculation of cross-sectional averages.\n",
        "4.  **Inference (Tasks 7-9):** It calculates the long-run response to debt, computes its standard error via the Delta Method, assigns significance stars, and compiles all results into a formatted table replicating Table 2.\n",
        "5.  **Validation (Tasks 10-11):** It runs a comprehensive set of robustness and sensitivity checks to confirm the stability of the main findings.\n",
        "6.  **Reporting (Tasks 12-14):** It synthesizes all quantitative outputs into a human-readable textual interpretation and a set of key visualizations.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `governments_reaction_public_debt_accumulation_draft.ipynb` notebook is structured as a logical pipeline with modular functions for each task:\n",
        "\n",
        "-   **Task 1: `validate_inputs`**: The initial quality gate for all inputs.\n",
        "-   **Task 2: `clean_and_prepare_data`**: Handles data quality issues.\n",
        "-   **Task 3: `set_panel_structure`**: Prepares the DataFrame for panel analysis.\n",
        "-   **Task 4: `generate_descriptive_statistics`**: Computes summary tables.\n",
        "-   **Task 5: `run_diagnostic_tests`**: Performs key econometric tests.\n",
        "-   **Task 6: `run_dccemg_estimation`**: The core estimation engine.\n",
        "-   **Task 7: `calculate_long_run_effects`**: Computes the key sustainability metric.\n",
        "-   **Task 8: `add_significance_indicators`**: Formats results for significance.\n",
        "-   **Task 9: `compile_results_table`**: Generates the final publication-quality table.\n",
        "-   **Task 10: `run_robustness_checks`**: Tests stability against specification changes.\n",
        "-   **Task 11: `run_sensitivity_analysis`**: Tests stability against parameter changes.\n",
        "-   **Task 12: `generate_results_interpretation`**: Creates a textual summary of findings.\n",
        "-   **Task 13: `generate_visualizations`**: Creates plots of key results.\n",
        "-   **Task 14: `create_fiscal_credibility_report`**: The top-level orchestrator that runs the entire workflow.\n",
        "\n",
        "## Key Callable: create_fiscal_credibility_report\n",
        "\n",
        "The central function in this project is `create_fiscal_credibility_report`. It orchestrates the entire analytical workflow from raw data to a final, comprehensive report object.\n",
        "\n",
        "```python\n",
        "def create_fiscal_credibility_report(\n",
        "    raw_df: pd.DataFrame,\n",
        "    analysis_parameters: Dict[str, Any],\n",
        "    enable_intensive_visuals: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete, end-to-end fiscal credibility analysis and compiles a master report.\n",
        "\n",
        "    This grand orchestrator function serves as the single entry point to run the\n",
        "    entire research project. It sequentially executes the baseline analysis,\n",
        "    a series of robustness and sensitivity checks, and finally generates\n",
        "    interpretive text and visualizations.\n",
        "\n",
        "    Args:\n",
        "        raw_df (pd.DataFrame): The raw input panel data.\n",
        "        analysis_parameters (Dict[str, Any]): A comprehensive dictionary containing all\n",
        "            parameters required for every stage of the analysis.\n",
        "        enable_intensive_visuals (bool): Flag to enable computationally expensive\n",
        "            visualizations. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A master dictionary containing the complete project results.\n",
        "    \"\"\"\n",
        "    # ... (implementation is in the notebook)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `statsmodels`, `linearmodels`, `scipy`, `matplotlib`, `seaborn`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/governments_reaction_public_debt_accumulation.git\n",
        "    cd governments_reaction_public_debt_accumulation\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy statsmodels linearmodels scipy matplotlib seaborn\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The primary input is a `pandas.DataFrame` with a specific schema. See the example usage section for a detailed breakdown and a script to generate a structurally correct synthetic dataset. The key columns are:\n",
        "-   `country_iso`, `year`\n",
        "-   `s_it` (primary surplus/GDP), `b_it` (debt/GDP), `a_it` (current account/GDP)\n",
        "-   `y_tilde_it` (output gap), `g_tilde_it` (spending gap)\n",
        "-   `s_it_lag1`, `b_it_lag1`\n",
        "-   `d_gfc_t`, `high_debt_indicator`, `industrial_indicator`\n",
        "-   **For sensitivity analysis:** `y_real` (log real GDP), `g_real` (log real gov't spending)\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `governments_reaction_public_debt_accumulation_draft.ipynb` notebook provides a complete, step-by-step guide. The core workflow is:\n",
        "\n",
        "1.  **Prepare Inputs:** Load your raw data into a `pandas.DataFrame` and define the `analysis_parameters` dictionary.\n",
        "2.  **Execute Pipeline:** Call the master orchestrator function:\n",
        "    ```python\n",
        "    master_report = create_fiscal_credibility_report(\n",
        "        raw_df=your_dataframe,\n",
        "        analysis_parameters=your_parameters\n",
        "    )\n",
        "    ```\n",
        "3.  **Inspect Outputs:** Programmatically access any result from the returned `master_report` dictionary. For example, to view the main results table:\n",
        "    ```python\n",
        "    final_table = master_report['baseline_analysis']['final_formatted_table']\n",
        "    print(final_table)\n",
        "    ```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `create_fiscal_credibility_report` function returns a single, comprehensive dictionary with the following top-level keys:\n",
        "\n",
        "-   `baseline_analysis`: Contains all artifacts from the main pipeline run (validation reports, analysis data, estimation results, final table).\n",
        "-   `robustness_checks`: Contains the final results tables from the various robustness scenarios.\n",
        "-   `sensitivity_analysis`: Contains the final results tables from the sensitivity scenarios.\n",
        "-   `textual_interpretation`: A markdown string summarizing the key findings from the baseline analysis.\n",
        "-   `visualizations`: A dictionary of `matplotlib` Figure objects for the key plots.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "governments_reaction_public_debt_accumulation/\n",
        "│\n",
        "├── governments_reaction_public_debt_accumulation_draft.ipynb  # Main implementation notebook\n",
        "├── requirements.txt                                             # Python package dependencies\n",
        "├── LICENSE                                                      # MIT license file\n",
        "└── README.md                                                    # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the master `analysis_parameters` dictionary. Users can easily modify:\n",
        "-   The `dcce_lags` for the main estimator.\n",
        "-   The exact list of `regressors` for any of the 10 models.\n",
        "-   The lists of countries in the `subsample_definitions`.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{canofari2025do,\n",
        "  title={Do Governments React to Public Debt Accumulation? A Cross-Country Analysis},\n",
        "  author={Canofari, Paolo and Piergallini, Alessandro and Tedeschi, Marco},\n",
        "  journal={arXiv preprint arXiv:2507.13084},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Python Implementation of \"Do Governments React to Public Debt Accumulation?\".\n",
        "GitHub repository: https://github.com/chirindaopensource/governments_reaction_public_debt_accumulation\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to Paolo Canofari, Alessandro Piergallini, and Marco Tedeschi for their rigorous and insightful research.\n",
        "-   Thanks to the developers of the `pandas`, `statsmodels`, `linearmodels`, and other scientific Python libraries that make this work possible.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `governments_reaction_public_debt_accumulation_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "nQh5vrJ1seFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Do Governments React to Public Debt Accumulation? A Cross-Country Analysis*\"\n",
        "\n",
        "Authors: Paolo Canofari, Alessandro Piergallini, Marco Tedeschi\n",
        "\n",
        "E-Journal Submission Date: 17th of July 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2507.13084\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Do governments adjust budgetary policy to rising public debt, precluding fiscal unsustainability? Using budget data for 52 industrial and emerging economies since 1990, we apply panel methods accounting for cross-sectional dependence and heterogeneous fiscal conduct. We find that a primary-balance rule with tax-smoothing motives and responsiveness to debt has robust explanatory power in describing fiscal behavior. Controlling for temporary output, temporary spending, and the current account balance, a 10-percentage-point increase in the debt-to-GDP ratio raises the long-run primary surplus-to-GDP ratio by 0.5 percentage points on average. Corrective adjustments hold across high- and low-debt countries and across industrial and emerging economies. Our results imply many governments pursue Ricardian policy designs, avoiding Ponzi-type financing.\n",
        "\n"
      ],
      "metadata": {
        "id": "chzAhuV3fDxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **Briefing on: \"Do Governments React to Public Debt Accumulation? A Cross-Country Analysis\"**\n",
        "\n",
        "This paper investigates a fundamental question in public finance: Is government fiscal policy sustainable? Specifically, do governments systematically adjust their primary budget balances in response to rising public debt, thereby ensuring they can meet their long-term obligations?\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 1: The Core Research Question and Its Significance**\n",
        "\n",
        "The central question is whether governments follow a \"responsible\" or \"Ricardian\" fiscal policy. In the wake of massive debt increases following the 2008 Global Financial Crisis and the COVID-19 pandemic, this question is not merely academic. An affirmative answer suggests that, on average, fiscal systems have a built-in stabilizer, preventing debt from spiraling out of control. A negative answer would imply that many countries are on an unsustainable path, risking \"Ponzi financing\" where new debt is issued merely to pay interest on old debt, a situation that cannot continue indefinitely.\n",
        "\n",
        "The authors test this by estimating a **fiscal reaction function** for a large panel of 52 industrial and emerging economies from 1990 to 2022.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 2: The Theoretical Framework: Bohn's Criterion and Tax Smoothing**\n",
        "\n",
        "The paper's empirical test is grounded in established macroeconomic theory.\n",
        "\n",
        "1.  **The Government's Intertemporal Budget Constraint:** The analysis starts with the standard law of motion for public debt. For fiscal policy to be sustainable, the present discounted value of all future primary surpluses must equal the current level of outstanding debt. This ensures the government's \"transversality condition\" is met, meaning debt does not grow faster than the economy's ability to service it.\n",
        "\n",
        "2.  **Bohn's (1998) Sustainability Criterion:** Direct tests of the intertemporal budget constraint are notoriously difficult. Henning Bohn proposed a simpler, more robust test: estimate a fiscal policy rule to see if the primary surplus (as a share of GDP) is a statistically significant, positive function of the lagged debt-to-GDP ratio. The rule they estimate is of the form:\n",
        "    *   *sᵢ,ₜ = φsᵢ,ₜ₋₁ + ρbᵢ,ₜ₋₁ + other controls + εᵢ,ₜ*\n",
        "    where:\n",
        "    *   *sᵢ,ₜ* is the primary surplus-to-GDP ratio for country *i* at time *t*.\n",
        "    *   *bᵢ,ₜ₋₁* is the lagged debt-to-GDP ratio.\n",
        "    *   *φ* captures policy inertia.\n",
        "    *   ***ρ*** **is the key parameter.** A positive and significant ***ρ*** implies that as debt rises, the government takes corrective action by increasing its primary surplus (either by cutting spending or raising taxes). This is the condition for sustainability.\n",
        "\n",
        "3.  **Barro's (1979) Tax Smoothing Hypothesis:** The \"other controls\" in the model are motivated by the theory of optimal fiscal policy. This theory suggests that governments should \"smooth\" tax rates over time to minimize economic distortions. Therefore, they should run deficits during temporary downturns (when output is low) or during periods of temporarily high spending (like wars or crises). The authors control for this by including:\n",
        "    *   **Temporary Output Gap:** The cyclical deviation of GDP from its trend.\n",
        "    *   **Temporary Spending Gap:** The cyclical deviation of government spending from its trend.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 3: The Methodological Contribution: Addressing Heterogeneity and Dependence**\n",
        "\n",
        "This is where the paper's econometric sophistication comes in. A simple panel regression would be invalid here for two reasons:\n",
        "\n",
        "1.  **Cross-Sectional Dependence:** Countries are not islands. A global shock (like the 2008 crisis or a pandemic) affects all economies simultaneously. This violates the standard assumption of independent errors across units.\n",
        "2.  **Slope Heterogeneity:** It is highly unlikely that Germany, Japan, and Nigeria all react to debt with the exact same coefficient (*ρ*). Each country has its own political institutions and economic structure. Assuming a common *ρ* for all countries is a restrictive and likely incorrect assumption.\n",
        "\n",
        "To address these critical issues, the authors employ the **Dynamic Common Correlated Effects Mean Group (DCCEMG)** estimator developed by Chudik and Pesaran (2015). This advanced technique explicitly models common unobserved factors (addressing dependence) and allows the policy coefficients (*ρ*, *φ*, etc.) to differ for each country, reporting the average effect (the \"mean group\" estimate). This makes the results far more robust and credible than those from older, simpler panel methods.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 4: Data and Key Diagnostic Tests**\n",
        "\n",
        "*   **Data:** The study uses a balanced panel of 52 countries (industrial and emerging) from 1990-2022, with data from the IMF, OECD, and World Bank.\n",
        "*   **Diagnostics (Table 1):** Before estimation, the authors run formal tests. They find strong evidence *against* slope homogeneity and *for* cross-sectional dependence. This formally justifies their choice of the DCCEMG estimator.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 5: The Main Empirical Findings (Table 2)**\n",
        "\n",
        "The core results are compelling and support the hypothesis of fiscal sustainability.\n",
        "\n",
        "*   **The Debt Response Coefficient (ρ):** The estimated average coefficient on lagged debt (*ρ*) is **0.033** and is highly statistically significant.\n",
        "*   **The Inertia Coefficient (φ):** The coefficient on the lagged primary surplus (*φ*) is **0.358**, indicating significant policy persistence.\n",
        "*   **The Long-Run Effect:** The long-run response of the primary surplus to debt is calculated as *ρ / (1 - φ)* = 0.033 / (1 - 0.358) ≈ **0.051**.\n",
        "*   **Economic Interpretation:** This means that for every **10 percentage point increase** in the debt-to-GDP ratio, governments, on average, increase their long-run primary surplus-to-GDP ratio by **0.51 percentage points**. This is a clear, economically meaningful corrective action.\n",
        "*   **Tax Smoothing:** The coefficients on the output gap (positive) and spending gap (negative) are significant and have the expected signs, confirming that governments also follow tax-smoothing principles.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 6: Nuanced Findings and Contribution to the Literature**\n",
        "\n",
        "The paper's most interesting contribution comes from splitting the sample, which allows it to engage with previous literature (e.g., Mendoza and Ostry, 2008), which found that the responsible fiscal behavior vanished for high-debt countries.\n",
        "\n",
        "*   **High-Debt vs. Low-Debt Countries:** The positive response to debt holds for **both groups**. The long-run response is weaker for high-debt countries (0.47) than for low-debt countries (0.65), but it remains statistically significant and positive. This is a crucial finding, suggesting that even highly indebted nations do not abandon fiscal discipline entirely.\n",
        "*   **Industrial vs. Emerging Economies:** The response also holds for both groups, but it is significantly stronger in industrial countries (0.60) than in emerging economies (0.27). This suggests that institutional quality and market access may play a role in the ability to make credible fiscal adjustments.\n",
        "*   **The 2008 Financial Crisis:** The crisis appears as a permanent negative shock to the *level* of the primary balance in high-debt countries, but it did **not** significantly change the *slope* coefficient (*ρ*). This means that while the crisis worsened the fiscal position, it did not break the fundamental corrective mechanism.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 7: Overall Conclusion and Implications**\n",
        "\n",
        "The paper concludes that, despite heterogeneity, a fundamental mechanism of fiscal correction is at work across a wide range of countries. The evidence strongly suggests that most governments pursue **Ricardian fiscal policies**, adjusting their budgets to ensure long-run solvency. They are not, on average, engaged in unsustainable Ponzi schemes.\n",
        "\n",
        "This provides a cautiously optimistic outlook on global fiscal health, indicating that the institutional frameworks in many countries are robust enough to trigger corrective actions when public debt levels become a concern. The paper is a strong example of how modern econometric methods can provide clearer and more reliable answers to long-standing questions in political economy."
      ],
      "metadata": {
        "id": "aSeB93JXfoUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "fZ3Q0iopDnKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "# =============================================================================\n",
        "#\n",
        "#  A Cross-Country Analysis of Government Fiscal Reaction Functions\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  econometric framework for analyzing fiscal sustainability, based on the\n",
        "#  methods presented in \"Do Governments React to Public Debt Accumulation?\n",
        "#  A Cross-Country Analysis\" by Canofari, Piergallini, and Tedeschi. It\n",
        "#  delivers a robust system for estimating sovereign fiscal policy rules,\n",
        "#  assessing their consistency with long-term solvency, and quantifying the\n",
        "#  heterogeneity of these rules across different country groups.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Estimation of dynamic fiscal policy reaction functions (Bohn, 1998).\n",
        "#  • Panel data diagnostics for slope heterogeneity (Swamy/Blomquist-Westerlund)\n",
        "#    and cross-sectional dependence (Pesaran CD).\n",
        "#  • Panel unit root testing in the presence of cross-sectional dependence (CIPS).\n",
        "#  • From-scratch implementation of the Dynamic Common Correlated Effects Mean\n",
        "#    Group (DCCEMG) estimator (Chudik & Pesaran, 2015) to account for\n",
        "#    unobserved common factors and heterogeneous policy responses.\n",
        "#  • Calculation of long-run fiscal response coefficients and their standard\n",
        "#    errors using the Delta Method.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • A modular, end-to-end pipeline from data validation to final report generation.\n",
        "#  • Rigorous subsample analysis with safeguards against data leakage.\n",
        "#  • Comprehensive robustness and sensitivity analysis framework.\n",
        "#  • Programmatic generation of publication-quality results tables, textual\n",
        "#    interpretations, and data visualizations.\n",
        "#  • Adherence to professional coding standards, including detailed type\n",
        "#    hinting, comprehensive docstrings, and line-by-line commenting.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Canofari, P., Piergallini, A., & Tedeschi, M. (2025). Do Governments React\n",
        "#  to Public Debt Accumulation? A Cross-Country Analysis.\n",
        "#  arXiv preprint arXiv:2507.13084v1 [econ.GN].\n",
        "#  https://arxiv.org/abs/2507.13084\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#  Date: 18 July 2025\n",
        "#\n",
        "# =============================================================================\n",
        "\n",
        "# =============================================================================\n",
        "# Standard Library Imports\n",
        "# =============================================================================\n",
        "import copy\n",
        "import traceback\n",
        "from typing import Any, Dict, List, Set, Tuple\n",
        "\n",
        "# =============================================================================\n",
        "# Third-Party Library Imports\n",
        "# =============================================================================\n",
        "# Core data manipulation and numerical operations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Econometrics and statistical modeling\n",
        "import statsmodels.api as sm\n",
        "from linearmodels.panel import PanelAR, PanelOLS\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import t as t_dist\n",
        "from scipy.stats.mstats import winsorize\n",
        "from statsmodels.tsa.filters import hpfilter\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.figure import Figure\n"
      ],
      "metadata": {
        "id": "6dYnxPZgDrtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "DvZpZkw9Dtn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### Discussion of Inputs, Processes and Outputs (IPO Analysis) of Key Callables\n",
        "\n",
        "\n",
        "#### **1. `validate_inputs`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df` (`pd.DataFrame`): The raw, unprocessed panel data in a long format.\n",
        "    *   `analysis_parameters` (`Dict`): The dictionary containing all study configurations.\n",
        "*   **Process:**\n",
        "    1.  **Schema Validation:** Compares the columns and data types of the input `df` against a predefined, required schema. It raises an error if columns are missing or extra, or if data types are incorrect.\n",
        "    2.  **Dimension Validation:** Checks if the panel is balanced and conforms to the study's dimensions (N=52 countries, T=33 years).\n",
        "    3.  **Consistency Validation:** Verifies that all country ISO codes in the data are present in the `analysis_parameters` and vice-versa.\n",
        "    4.  **Parameter Validation:** Inspects the structure and content of the `analysis_parameters` dictionary to ensure its integrity.\n",
        "*   **Data Transformation:** The primary transformation is the coercion of data types to match the required schema (e.g., converting a float 'year' column to an integer). It operates on a copy of the input data.\n",
        "*   **Outputs:**\n",
        "    *   `df_validated` (`pd.DataFrame`): A validated copy of the input DataFrame with corrected data types.\n",
        "    *   `validation_report` (`Dict`): A dictionary detailing the success or failure of each validation step.\n",
        "*   **Role in Research:** This function serves as the foundational data integrity check. While not explicitly a numbered equation, it ensures that the data used for the entire analysis is sound, complete, and consistent with the authors' sample, as described in **Section 3: Data**. It is the first gatekeeper of quality control.\n",
        "\n",
        "--\n",
        "\n",
        "#### **2. `clean_and_prepare_data`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `validated_df` (`pd.DataFrame`): The DataFrame that has passed the checks from `validate_inputs`.\n",
        "*   **Process:**\n",
        "    1.  **Missing Value Analysis:** Confirms that `NaN`s exist only for lagged variables in the first year of the sample (1990) and then removes these rows. This prepares the data for dynamic panel estimation.\n",
        "    2.  **Outlier Treatment:** Clips specified continuous variables to within plausible economic bounds to mitigate the influence of extreme, non-representative data points.\n",
        "    3.  **Consistency Validation:** Verifies the logical integrity of constructed variables, such as the `d_gfc_t` dummy and the time-invariance of country-specific indicators.\n",
        "    4.  **Range Validation:** Checks that the cyclical components (`y_tilde_it`, `g_tilde_it`), which are inputs to the model, are centered around zero, a key property of the HP filter output mentioned in **Section 3: Data**.\n",
        "*   **Data Transformation:** This function performs two key transformations: it removes the first year of observations containing `NaN`s and clips the values of specified columns.\n",
        "*   **Outputs:**\n",
        "    *   `df_clean` (`pd.DataFrame`): The cleansed DataFrame, ready for structuring.\n",
        "    *   `cleansing_report` (`Dict`): A dictionary providing an audit trail of the cleansing process (e.g., number of rows dropped, number of values clipped).\n",
        "*   **Role in Research:** This function implements the data preparation steps that are standard practice in empirical macroeconomics but often only briefly mentioned. It ensures the data is clean and robust before any statistical analysis is performed, directly supporting the quality of the results for the main estimation of Equation (5).\n",
        "\n",
        "--\n",
        "\n",
        "#### **3. `set_panel_structure`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `cleansed_df` (`pd.DataFrame`): The DataFrame output from `clean_and_prepare_data`.\n",
        "*   **Process:**\n",
        "    1.  **MultiIndex Creation:** Sets the `['country_iso', 'year']` columns as the DataFrame's index.\n",
        "    2.  **Sorting:** Sorts the new `MultiIndex` to ensure that data for each country is in chronological order.\n",
        "    3.  **Validation:** Performs post-condition checks to confirm the index is unique and monotonically increasing.\n",
        "*   **Data Transformation:** This is a purely structural transformation. It changes the DataFrame's indexing from a standard integer index to a hierarchical `MultiIndex`, which is the canonical format for panel data in Python.\n",
        "*   **Outputs:**\n",
        "    *   `panel_df` (`pd.DataFrame`): The same data, but now structured with a `MultiIndex` for efficient panel operations.\n",
        "*   **Role in Research:** This function prepares the data for the specific requirements of panel data econometrics. A correctly sorted `MultiIndex` is a prerequisite for correctly calculating lags, cross-sectional averages, and running panel regressions as specified in the DCCEMG methodology used to estimate **Equation (5)**.\n",
        "\n",
        "--\n",
        "\n",
        "#### **4. `generate_descriptive_statistics`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `panel_df` (`pd.DataFrame`): The panel-structured DataFrame.\n",
        "*   **Process:**\n",
        "    1.  **Summary Statistics:** Computes a detailed summary (mean, std, extended percentiles, skew, kurtosis) for all variables in the full panel.\n",
        "    2.  **Subsample Statistics:** Repeats the summary statistics calculation for each of the four subsamples (high/low debt, industrial/emerging).\n",
        "    3.  **Correlation Analysis:** Computes Pearson, Spearman, and Kendall correlation matrices for the full panel.\n",
        "    4.  **Visualization Data Prep:** Calculates the yearly cross-sectional averages for the full panel and all subsamples.\n",
        "*   **Data Transformation:** This function does not transform the input DataFrame. It is an analytical function that reads the data and produces new DataFrames containing statistical summaries.\n",
        "*   **Outputs:**\n",
        "    *   `results` (`Dict`): A nested dictionary containing all the generated statistical tables (as DataFrames).\n",
        "*   **Role in Research:** This function generates the quantitative summaries that form the basis of **Table 1: Summary and diagnostic tests** and the data needed to plot **Figure 1: Primary balance and public debt** and **Figure 2: Temporary output and temporary spending**.\n",
        "\n",
        "--\n",
        "\n",
        "#### **5. `run_diagnostic_tests`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `panel_df` (`pd.DataFrame`): The panel-structured DataFrame.\n",
        "    *   `analysis_parameters` (`Dict`): The configuration dictionary.\n",
        "*   **Process:**\n",
        "    1.  **Slope Homogeneity Test:** Performs an F-test for poolability (a test for `H₀: βᵢ = β`), which is the key diagnostic justifying the use of a Mean Group estimator over a pooled or fixed-effects model.\n",
        "    2.  **Pesaran CD Test:** Tests for cross-sectional dependence in the model's residuals. This is a critical test because its result dictates whether standard panel estimators are valid. It implements the formula:\n",
        "        $$\n",
        "        CD = \\sqrt{\\frac{2T}{N(N-1)}} \\left( \\sum_{i=1}^{N-1} \\sum_{j=i+1}^{N} \\hat{\\rho}_{ij} \\right)\n",
        "        $$\n",
        "    3.  **CIPS Unit Root Test:** Tests for unit roots in the key series (`s_it`, `b_it_lag1`) using a method robust to cross-sectional dependence. This is based on averaging individual Cross-sectionally Augmented Dickey-Fuller (CADF) regressions.\n",
        "*   **Data Transformation:** The function generates a series of residuals from a pooled OLS regression of Equation (5) to be used as input for the CD test.\n",
        "*   **Outputs:**\n",
        "    *   `diagnostic_results` (`Dict`): A nested dictionary containing the test statistic, p-value, and interpretation for each diagnostic test.\n",
        "*   **Role in Research:** This function programmatically generates the results for the \"Diagnostic statistics\" section of **Table 1**. The outcomes of these tests (rejection of slope homogeneity, presence of cross-sectional dependence) provide the explicit econometric justification for choosing the advanced DCCEMG estimator to estimate **Equation (5)**.\n",
        "\n",
        "--\n",
        "\n",
        "#### **6. `run_dccemg_estimation` (and its helper `_estimate_single_dccemg_model`)**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `panel_df` (`pd.DataFrame`): The panel-structured DataFrame.\n",
        "    *   `analysis_parameters` (`Dict`): The configuration dictionary.\n",
        "*   **Process:** This function is the core of the analysis. For each of the 10 model specifications:\n",
        "    1.  **Subsample Preparation:** It selects the correct subset of countries.\n",
        "    2.  **CCE Proxy Generation:** It computes the cross-sectional averages of all model variables *for that subsample only* and generates their lags (from 0 to `p=3`).\n",
        "    3.  **Individual Regressions:** It iterates through each country in the subsample and estimates the augmented regression via OLS:\n",
        "        $$\n",
        "        s_{it} = \\phi_i s_{i,t-1} + \\rho_i b_{i,t-1} + \\boldsymbol{\\beta_i' x_{it}} + \\sum_{m=0}^{p} \\boldsymbol{\\delta_{im}' \\bar{w}_{t-m}} + \\epsilon_{it}\n",
        "        $$\n",
        "        where `x_it` are the other regressors and `w̄_t` are the cross-sectional averages.\n",
        "    4.  **Mean Group Aggregation:** It calculates the Mean Group estimate for each coefficient by taking the simple average of the individual country coefficients (`β̂_MG = (1/Nₛ) * Σᵢ∈ₛ β̂ᵢ`).\n",
        "    5.  **Standard Error Calculation:** It calculates the standard error of the Mean Group estimate.\n",
        "*   **Data Transformation:** This function performs extensive data transformation by creating the CCE proxy variables for each of the 10 models.\n",
        "*   **Outputs:**\n",
        "    *   `all_model_results` (`Dict`): A nested dictionary containing the detailed estimation results (coefficients, SEs, etc.) and the DataFrame of individual country coefficients for all 10 models.\n",
        "*   **Role in Research:** This function is the direct implementation of the paper's core empirical strategy. It estimates the fiscal reaction function, **Equation (5)**, using the DCCEMG method, producing all the numerical results that are presented in **Table 2: Determinants of the primary balance-to-GDP ratio**.\n",
        "\n",
        "--\n",
        "\n",
        "#### **7. `calculate_long_run_effects`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `estimation_results` (`Dict`): The dictionary of results from `run_dccemg_estimation`.\n",
        "*   **Process:**\n",
        "    1.  **LRR Calculation:** For each model, it calculates the long-run response of the primary surplus to debt using the formula derived from Equation (4):\n",
        "        $$\n",
        "        LRR = \\frac{\\rho_{MG}}{1 - \\phi_{MG}}\n",
        "        $$\n",
        "    2.  **Standard Error Calculation:** It computes the standard error of the LRR using the Delta Method, which requires the variance and covariance of the `ρ_MG` and `φ_MG` estimators.\n",
        "*   **Data Transformation:** This function does not transform data but rather transforms the results. It augments the `estimation_results` dictionary by adding a new sub-dictionary, `'long_run_effects'`, to each model's results.\n",
        "*   **Outputs:** None (it modifies the input dictionary in-place).\n",
        "*   **Role in Research:** This function calculates a key metric for interpreting the results of Table 2. The LRR is the central measure of fiscal sustainability discussed throughout the paper, for example in the **Abstract**: \"...a 10-percentage-point increase in the debt-to-GDP ratio raises the long-run primary surplus-to-GDP ratio by 0.5 percentage points on average.\"\n",
        "\n",
        "--\n",
        "\n",
        "#### **8. `add_significance_indicators`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `estimation_results` (`Dict`): The dictionary of results, now including long-run effects.\n",
        "*   **Process:**\n",
        "    1.  It iterates through each model's results.\n",
        "    2.  Using the p-values for the coefficients and the long-run effect, it assigns the conventional significance stars (`*`, `**`, `***`) based on standard thresholds (10%, 5%, 1%).\n",
        "*   **Data Transformation:** It augments the `estimation_results` dictionary by adding a `'significance'` key containing the star strings to each model's results and its `'long_run_effects'` sub-dictionary.\n",
        "*   **Outputs:** None (it modifies the input dictionary in-place).\n",
        "*   **Role in Research:** This is a formatting step that prepares the numerical results for presentation in the standard academic format seen in **Table 2**.\n",
        "\n",
        "--\n",
        "\n",
        "#### **9. `compile_results_table`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `estimation_results` (`Dict`): The fully populated results dictionary.\n",
        "    *   `analysis_parameters` (`Dict`): The configuration dictionary.\n",
        "*   **Process:**\n",
        "    1.  It iterates through an ordered list of variables and models.\n",
        "    2.  For each cell, it extracts the coefficient, standard error, and significance star.\n",
        "    3.  It formats these pieces of information into the standard presentation style (e.g., \"0.033***\", \"(0.0081)\").\n",
        "    4.  It assembles these formatted strings into a `pandas.DataFrame`.\n",
        "*   **Data Transformation:** This is a pure transformation function, converting the nested dictionary of numerical results into a single, formatted `DataFrame` of strings.\n",
        "*   **Outputs:**\n",
        "    *   `final_table` (`pd.DataFrame`): The publication-quality results table.\n",
        "*   **Role in Research:** This function is the final step in producing the paper's main empirical output: **Table 2**.\n",
        "\n",
        "--\n",
        "\n",
        "#### **10. `run_robustness_checks`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `raw_df` (`pd.DataFrame`): The original, raw data.\n",
        "    *   `analysis_parameters` (`Dict`): The baseline configuration.\n",
        "*   **Process:**\n",
        "    1.  It systematically creates modified versions of the inputs (e.g., a DataFrame with a different GFC dummy, or a parameter dictionary with a different lag length).\n",
        "    2.  For each modification, it calls the main orchestrator, `run_fiscal_sustainability_analysis`, to re-run the entire pipeline from start to finish.\n",
        "*   **Data Transformation:** It creates temporary, modified copies of the input data and parameters for each check.\n",
        "*   **Outputs:**\n",
        "    *   `robustness_results` (`Dict`): A dictionary where each key represents a specific check and the value is the final formatted results table from that run.\n",
        "*   **Role in Research:** This function programmatically performs the kind of robustness analysis that is essential for a credible empirical study, ensuring the main findings in **Table 2** are not sensitive to arbitrary modeling choices.\n",
        "\n",
        "--\n",
        "\n",
        "#### **11. `run_sensitivity_analysis`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `raw_df` (`pd.DataFrame`): The original data, which must include unfiltered series.\n",
        "    *   `analysis_parameters` (`Dict`): The baseline configuration.\n",
        "*   **Process:**\n",
        "    1.  It systematically creates modified versions of the inputs by altering key data definitions (e.g., re-calculating cyclical gaps with a new HP filter lambda, re-defining high-debt countries based on terciles).\n",
        "    2.  For each modification, it calls the main orchestrator, `run_fiscal_sustainability_analysis`.\n",
        "*   **Data Transformation:** It performs significant data transformations on copies of the raw data before running the main pipeline.\n",
        "*   **Outputs:**\n",
        "    *   `sensitivity_results` (`Dict`): A dictionary containing the final formatted results table for each sensitivity run.\n",
        "*   **Role in Research:** This function explores the impact of key definitional assumptions on the results, providing deeper insight into the stability of the conclusions drawn from **Table 2**.\n",
        "\n",
        "--\n",
        "\n",
        "#### **12. `generate_results_interpretation`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `estimation_results` (`Dict`): The results from the *baseline* analysis.\n",
        "*   **Process:**\n",
        "    1.  It extracts key numerical results (coefficients, LRR, p-values) for different models.\n",
        "    2.  It uses f-strings and conditional logic to embed these numbers into a pre-defined narrative structure.\n",
        "    3.  It performs simple calculations to quantify economic significance and compare coefficients across groups.\n",
        "*   **Data Transformation:** It transforms numerical data into a structured, human-readable markdown string.\n",
        "*   **Outputs:**\n",
        "    *   `report` (`str`): A formatted markdown string.\n",
        "*   **Role in Research:** This function programmatically generates the textual analysis that would appear in **Section 4: Empirical Results** and **Section 5: Conclusion**, where the authors discuss the meaning and implications of the numbers in Table 2.\n",
        "\n",
        "--\n",
        "\n",
        "#### **13. `generate_visualizations`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `estimation_results` (`Dict`): The baseline results.\n",
        "    *   `analysis_dataframe` (`pd.DataFrame`): The main data used for the baseline analysis.\n",
        "*   **Process:**\n",
        "    1.  **Coefficient Plot:** Extracts coefficients and standard errors to create a forest plot comparing the debt response across subsamples.\n",
        "    2.  **Scatter Plot:** Uses the raw data to create faceted scatter plots showing the relationship between debt and primary balance for different groups.\n",
        "    3.  **Rolling Plot:** (Optional) Performs a computationally intensive rolling-window estimation to show the time-variation of the key coefficient.\n",
        "*   **Data Transformation:** It transforms data from the results dictionary and the main DataFrame into formats suitable for the plotting libraries.\n",
        "*   **Outputs:**\n",
        "    *   `figures` (`Dict`): A dictionary where keys are plot names and values are `matplotlib` Figure objects.\n",
        "*   **Role in Research:** This function generates visualizations that would be analogous to **Figure 1** and other potential plots used to illustrate the paper's key findings and the heterogeneity in the data.\n",
        "\n",
        "--\n",
        "\n",
        "#### **14. `create_fiscal_credibility_report`**\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `raw_df` (`pd.DataFrame`): The original, raw data.\n",
        "    *   `analysis_parameters` (`Dict`): The baseline configuration.\n",
        "*   **Process:** This is the grand orchestrator. It executes the entire workflow in the correct sequence:\n",
        "    1.  Calls `run_fiscal_sustainability_analysis` to get the baseline results.\n",
        "    2.  Calls `run_robustness_checks`.\n",
        "    3.  Calls `run_sensitivity_analysis`.\n",
        "    4.  Calls `generate_results_interpretation` on the baseline results.\n",
        "    5.  Calls `generate_visualizations` on the baseline results.\n",
        "    6.  Compiles all outputs into a single, master dictionary.\n",
        "*   **Data Transformation:** It manages the flow of data between all the major components of the pipeline.\n",
        "*   **Outputs:**\n",
        "    *   `master_report` (`Dict`): The final, all-encompassing dictionary containing every artifact generated by the entire project.\n",
        "*   **Role in Research:** This function represents the entire research project in a single callable. Executing it is equivalent to running the Stata `.do` file or R script that would replicate the entire paper from raw data to final outputs.\n",
        "\n",
        "### Usage Example\n",
        "\n",
        "This example demonstrates how a quantitative analyst would use the master orchestrator function `create_fiscal_credibility_report` to run the entire research project, from raw data to a complete set of results, tables, and visualizations.\n",
        "\n",
        "#### **Step 1: Assemble the Required Inputs**\n",
        "\n",
        "The master function requires two primary inputs: the raw data in a `pandas.DataFrame` and the analysis parameters in a Python `dict`.\n",
        "\n",
        "**Input 1: The Raw Data (`raw_df`)**\n",
        "\n",
        "First, we must load or create the raw dataset. For this example, we will generate a synthetic but structurally correct DataFrame. In a real-world scenario, this data would be meticulously assembled from sources like the IMF, World Bank, and OECD.\n",
        "\n",
        "*   **Crucial Note:** For the sensitivity analysis on the HP filter to run, the raw DataFrame must contain the unfiltered, real log series for GDP (`y_real`) and government spending (`g_real`). The other cyclical columns (`y_tilde_it`, `g_tilde_it`) represent the *baseline* filtered data.\n",
        "\n",
        "```python\n",
        "# Python Code Snippet: Creating a synthetic but structurally correct raw DataFrame\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the panel dimensions from the paper\n",
        "countries = [\n",
        "    'AUT', 'BEL', 'BRA', 'CAN', 'CHN', 'HRV', 'EGY', 'FIN', 'FRA', 'DEU',\n",
        "    'GRC', 'HUN', 'ISL', 'IND', 'ISR', 'ITA', 'JPN', 'JOR', 'MYS', 'MAR',\n",
        "    'PRT', 'ZAF', 'ESP', 'UKR', 'GBR', 'USA', 'AUS', 'BGR', 'CHL', 'COL',\n",
        "    'CIV', 'DNK', 'IDN', 'IRL', 'KOR', 'LUX', 'MEX', 'NLD', 'NZL', 'NGA',\n",
        "    'NOR', 'PAN', 'PRY', 'PER', 'PHL', 'POL', 'ROU', 'RUS', 'SWE', 'CHE',\n",
        "    'THA', 'URY'\n",
        "]\n",
        "years = range(1990, 2023) # T=33 years\n",
        "\n",
        "# Create the MultiIndex\n",
        "index = pd.MultiIndex.from_product([countries, years], names=['country_iso', 'year'])\n",
        "\n",
        "# Create the DataFrame\n",
        "raw_df = pd.DataFrame(index=index).reset_index()\n",
        "\n",
        "# --- Generate synthetic data for all required columns ---\n",
        "# This simulates the data loading process.\n",
        "np.random.seed(42)\n",
        "n_obs = len(raw_df)\n",
        "\n",
        "# Core variables\n",
        "raw_df['s_it'] = np.random.normal(-0.01, 0.03, n_obs)\n",
        "raw_df['b_it'] = np.random.gamma(4, 15, n_obs) / 100\n",
        "raw_df['a_it'] = np.random.normal(0, 0.05, n_obs)\n",
        "\n",
        "# Unfiltered series for sensitivity analysis (as log levels)\n",
        "raw_df['y_real'] = raw_df.groupby('country_iso')['year'].transform(lambda x: 10 + 0.02 * (x - 1990)) + np.random.normal(0, 0.02, n_obs).cumsum()\n",
        "raw_df['g_real'] = raw_df.groupby('country_iso')['year'].transform(lambda x: 5 + 0.02 * (x - 1990)) + np.random.normal(0, 0.02, n_obs).cumsum()\n",
        "\n",
        "# Baseline cyclical components (pre-calculated with lambda=100)\n",
        "raw_df['y_tilde_it'] = raw_df.groupby('country_iso')['y_real'].transform(lambda x: hpfilter(x, lamb=100)[1])\n",
        "raw_df['g_tilde_it'] = raw_df.groupby('country_iso')['g_real'].transform(lambda x: hpfilter(x, lamb=100)[1])\n",
        "\n",
        "# Lagged variables (will have NaNs in 1990)\n",
        "raw_df['s_it_lag1'] = raw_df.groupby('country_iso')['s_it'].shift(1)\n",
        "raw_df['b_it_lag1'] = raw_df.groupby('country_iso')['b_it'].shift(1)\n",
        "\n",
        "# Dummy variables\n",
        "raw_df['d_gfc_t'] = (raw_df['year'] >= 2008).astype(int)\n",
        "\n",
        "# Country-specific, time-invariant indicators\n",
        "median_debt = raw_df.groupby('country_iso')['b_it'].transform('median')\n",
        "overall_median = median_debt.median()\n",
        "raw_df['high_debt_indicator'] = (median_debt > overall_median).astype(int)\n",
        "\n",
        "industrial_countries = [\n",
        "    'AUS', 'AUT', 'BEL', 'CAN', 'CHN', 'DNK', 'FIN', 'FRA', 'DEU', 'GRC',\n",
        "    'ISL', 'IRL', 'ISR', 'ITA', 'JPN', 'KOR', 'LUX', 'NLD', 'NZL', 'NOR',\n",
        "    'POL', 'ESP', 'SWE', 'CHE', 'GBR', 'USA'\n",
        "]\n",
        "raw_df['industrial_indicator'] = raw_df['country_iso'].isin(industrial_countries).astype(int)\n",
        "\n",
        "print(\"Synthetic raw DataFrame created with shape:\", raw_df.shape)\n",
        "print(raw_df.head())\n",
        "```\n",
        "\n",
        "**Input 2: The Analysis Parameters (`analysis_parameters`)**\n",
        "\n",
        "Next, we define the configuration dictionary. This dictionary controls every aspect of the analysis, from algorithmic parameters to model specifications and subsample definitions. Using the default version from the original prompt ensures that we are replicating the paper's baseline analysis.\n",
        "\n",
        "```python\n",
        "# Python Code Snippet: Defining the analysis_parameters dictionary\n",
        "\n",
        "analysis_parameters = {\n",
        "    \"hp_filter_lambda\": {\n",
        "        \"value\": 100,\n",
        "        \"source\": \"Section 3: Data, Page 8\",\n",
        "        \"description\": \"The smoothing parameter (lambda) for the Hodrick-Prescott filter, standard for annual data.\"\n",
        "    },\n",
        "    \"dcce_lags\": {\n",
        "        \"value\": 3,\n",
        "        \"source\": \"Footnote 5, Page 7\",\n",
        "        \"description\": \"The number of lags (p) of the cross-sectional averages to include in the DCCE augmented regression.\"\n",
        "    },\n",
        "    \"model_specifications\": {\n",
        "        \"col_1\": {\"description\": \"Aggregate Panel, Base Model\", \"regressors\": ['s_it_lag1', 'b_it_lag1', 'y_tilde_it', 'g_tilde_it', 'd_gfc_t']},\n",
        "        \"col_2\": {\"description\": \"Aggregate Panel, Full Model\", \"regressors\": ['s_it_lag1', 'b_it_lag1', 'y_tilde_it', 'g_tilde_it', 'a_it', 'd_gfc_t']},\n",
        "        \"col_3\": {\"description\": \"High-Debt Countries, Base Model\", \"regressors\": ['s_it_lag1', 'b_it_lag1', 'y_tilde_it', 'g_tilde_it', 'd_gfc_t']},\n",
        "        \"col_4\": {\"description\": \"High-Debt Countries, Full Model\", \"regressors\": ['s_it_lag1', 'b_it_lag1', 'y_tilde_it', 'g_tilde_it', 'a_it', 'd_gfc_t']},\n",
        "        \"col_5\": {\"description\": \"Low-Debt Countries, Base Model\", \"regressors\": ['s_it_lag1', 'b_it_lag1', 'y_tilde_it', 'g_tilde_it', 'd_gfc_t']},\n",
        "        \"col_6\": {\"description\": \"Low-Debt Countries, Full Model\", \"regressors\": ['s_it_lag1', 'b_it_lag1', 'y_tilde_it', 'g_tilde_it', 'a_it', 'd_gfc_t']},\n",
        "        \"col_7\": {\"description\": \"Industrial Countries, Base Model\", \"regressors\": ['s_it_lag1', 'b_it_lag1', 'y_tilde_it', 'g_tilde_it', 'd_gfc_t']},\n",
        "        \"col_8\": {\"description\": \"Industrial Countries, Full Model\", \"regressors\": ['s_it_lag1', 'b_it_lag1', 'y_tilde_it', 'g_tilde_it', 'a_it', 'd_gfc_t']},\n",
        "        \"col_9\": {\"description\": \"Emerging Countries, Base Model\", \"regressors\": ['s_it_lag1', 'b_it_lag1', 'y_tilde_it', 'g_tilde_it', 'd_gfc_t']},\n",
        "        \"col_10\": {\"description\": \"Emerging Countries, Full Model\", \"regressors\": ['s_it_lag1', 'b_it_lag1', 'y_tilde_it', 'g_tilde_it', 'a_it', 'd_gfc_t']}\n",
        "    },\n",
        "    \"subsample_definitions\": {\n",
        "        \"high_debt\": ['AUT', 'BEL', 'BRA', 'CAN', 'CHN', 'HRV', 'EGY', 'FIN', 'FRA', 'DEU', 'GRC', 'HUN', 'ISL', 'IND', 'ISR', 'ITA', 'JPN', 'JOR', 'MYS', 'MAR', 'PRT', 'ZAF', 'ESP', 'UKR', 'GBR', 'USA'],\n",
        "        \"low_debt\": ['AUS', 'BGR', 'CHL', 'COL', 'CIV', 'DNK', 'IDN', 'IRL', 'KOR', 'LUX', 'MEX', 'NLD', 'NZL', 'NGA', 'NOR', 'PAN', 'PRY', 'PER', 'PHL', 'POL', 'ROU', 'RUS', 'SWE', 'CHE', 'THA', 'URY'],\n",
        "        \"industrial\": ['AUS', 'AUT', 'BEL', 'CAN', 'CHN', 'DNK', 'FIN', 'FRA', 'DEU', 'GRC', 'ISL', 'IRL', 'ISR', 'ITA', 'JPN', 'KOR', 'LUX', 'NLD', 'NZL', 'NOR', 'POL', 'ESP', 'SWE', 'CHE', 'GBR', 'USA'],\n",
        "        \"emerging\": ['BRA', 'BGR', 'CHL', 'COL', 'CIV', 'HRV', 'EGY', 'HUN', 'IND', 'IDN', 'JOR', 'MYS', 'MEX', 'MAR', 'NGA', 'PAN', 'PRY', 'PER', 'PHL', 'PRT', 'ROU', 'RUS', 'ZAF', 'THA', 'UKR']\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nAnalysis parameters dictionary defined.\")\n",
        "```\n",
        "\n",
        "#### **Step 2: Execute the Master Orchestrator Function**\n",
        "\n",
        "With the inputs prepared, executing the entire analysis is a single function call. We pass the `raw_df` and `analysis_parameters` to `create_fiscal_credibility_report`. We can also decide whether to enable the computationally expensive visualizations. For this demonstration, we will leave it as `False`.\n",
        "\n",
        "```python\n",
        "# Python Code Snippet: Running the full pipeline\n",
        "\n",
        "# Ensure all the developed functions (from validate_inputs to the orchestrator)\n",
        "# are defined in the current scope before running this.\n",
        "\n",
        "# Execute the entire end-to-end pipeline.\n",
        "master_report = create_fiscal_credibility_report(\n",
        "    raw_df=raw_df,\n",
        "    analysis_parameters=analysis_parameters,\n",
        "    enable_intensive_visuals=False  # Set to True to run the rolling-window plot\n",
        ")\n",
        "```\n",
        "\n",
        "#### **Step 3: Inspect the Outputs**\n",
        "\n",
        "The `master_report` dictionary now contains every artifact from the entire project. An analyst can now easily access any piece of the analysis for review, further processing, or export.\n",
        "\n",
        "```python\n",
        "# Python Code Snippet: Exploring the master_report dictionary\n",
        "\n",
        "# Check the top-level keys of the output dictionary\n",
        "print(\"\\nTop-level keys in the master report:\", list(master_report.keys()))\n",
        "\n",
        "# --- Example 1: Display the final, publication-quality results table ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Displaying Final Formatted Results Table (Baseline Analysis)\")\n",
        "print(\"=\"*50)\n",
        "final_table = master_report.get('baseline_analysis', {}).get('final_formatted_table')\n",
        "if isinstance(final_table, pd.DataFrame):\n",
        "    # Using pandas display options for better console output\n",
        "    with pd.option_context('display.max_rows', None, 'display.width', 1000):\n",
        "        print(final_table)\n",
        "else:\n",
        "    print(\"Final table not available.\")\n",
        "\n",
        "# --- Example 2: Display the textual interpretation ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Displaying Programmatically Generated Interpretation\")\n",
        "print(\"=\"*50)\n",
        "interpretation = master_report.get('textual_interpretation')\n",
        "if interpretation:\n",
        "    print(interpretation)\n",
        "else:\n",
        "    print(\"Interpretation not available.\")\n",
        "\n",
        "# --- Example 3: Access a specific robustness check result ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Displaying a Specific Robustness Check Result (Alternative Lags = 2)\")\n",
        "print(\"=\"*50)\n",
        "robustness_table_lags_2 = master_report.get('robustness_checks', {}).get('alternative_lags', {}).get('lags_2')\n",
        "if isinstance(robustness_table_lags_2, pd.DataFrame):\n",
        "    with pd.option_context('display.max_rows', None, 'display.width', 1000):\n",
        "        print(robustness_table_lags_2)\n",
        "else:\n",
        "    print(\"Robustness check for 2 lags not available or failed.\")\n",
        "\n",
        "# --- Example 4: Access a visualization object ---\n",
        "# Note: The figure object can be saved or displayed.\n",
        "visualizations = master_report.get('visualizations', {})\n",
        "coeff_plot_fig = visualizations.get('coefficient_forest_plot')\n",
        "\n",
        "if isinstance(coeff_plot_fig, Figure):\n",
        "    print(\"\\nCoefficient forest plot Figure object is available.\")\n",
        "    # To save the figure:\n",
        "    # coeff_plot_fig.savefig(\"coefficient_forest_plot.png\", dpi=300)\n",
        "    # print(\"Figure saved to 'coefficient_forest_plot.png'\")\n",
        "else:\n",
        "    print(\"\\nCoefficient forest plot not available.\")\n",
        "```\n",
        "\n",
        "This example provides a complete, workflow for using the analytical pipeline. It demonstrates the clear separation of data, parameters, and execution, and shows how the final, output object can be easily navigated to access any desired result.\n",
        "\n",
        "\n",
        "\n",
        "     \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e1MAaa9LDvh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Data and Parameter Validation\n",
        "\n",
        "def validate_inputs(\n",
        "    df: pd.DataFrame,\n",
        "    analysis_parameters: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs a comprehensive validation of the input DataFrame and parameter\n",
        "    dictionary, ensuring they conform to the stringent requirements of the\n",
        "    research methodology.\n",
        "\n",
        "    This function executes the four key validation steps:\n",
        "    1.  DataFrame Structure Validation: Checks for required columns, correct\n",
        "        data types, and absence of extraneous columns.\n",
        "    2.  Panel Dimension Validation: Verifies the panel is balanced with exactly\n",
        "        N=52 countries and T=33 years (1990-2022).\n",
        "    3.  Country Code Consistency: Ensures all country ISO codes in the data\n",
        "        match those specified in the analysis parameters.\n",
        "    4.  Parameter Dictionary Validation: Validates the structure and content of\n",
        "        the analysis_parameters dictionary itself.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame):\n",
        "            The input panel data. Must be in a \"long\" format with columns for\n",
        "            'country_iso' and 'year'.\n",
        "        analysis_parameters (Dict[str, Any]):\n",
        "            A dictionary containing all algorithmic, model, and subsample\n",
        "            specifications for the analysis.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - A validated and cleaned copy of the input DataFrame.\n",
        "            - A detailed validation report dictionary summarizing the results\n",
        "              of each validation step.\n",
        "\n",
        "    Raises:\n",
        "        ValueError:\n",
        "            If any critical validation check fails, such as missing columns,\n",
        "            incorrect dimensions, or inconsistent country codes.\n",
        "        TypeError:\n",
        "            If the inputs are not of the expected type (e.g., df is not a\n",
        "            pandas DataFrame).\n",
        "    \"\"\"\n",
        "    # =========================================================================\n",
        "    # 0. Initial Input Type Validation\n",
        "    # =========================================================================\n",
        "    # Ensure the primary inputs are of the correct type before any processing.\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        # Raise a TypeError if the 'df' argument is not a pandas DataFrame.\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # Ensure the 'analysis_parameters' argument is a dictionary.\n",
        "    if not isinstance(analysis_parameters, dict):\n",
        "        # Raise a TypeError if the parameters are not in a dictionary.\n",
        "        raise TypeError(\"Input 'analysis_parameters' must be a dictionary.\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # 1. DataFrame Structure Validation (Task 1, Step 1)\n",
        "    # =========================================================================\n",
        "    # Initialize the validation report dictionary.\n",
        "    validation_report: Dict[str, Any] = {}\n",
        "\n",
        "    # Create a copy of the DataFrame to avoid modifying the original object.\n",
        "    df_validated = df.copy()\n",
        "\n",
        "    # Define the required schema with column names and expected data types.\n",
        "    expected_schema = {\n",
        "        'country_iso': 'object',\n",
        "        'year': 'int64',\n",
        "        's_it': 'float64',\n",
        "        'b_it': 'float64',\n",
        "        'a_it': 'float64',\n",
        "        'y_tilde_it': 'float64',\n",
        "        'g_tilde_it': 'float64',\n",
        "        's_it_lag1': 'float64',\n",
        "        'b_it_lag1': 'float64',\n",
        "        'd_gfc_t': 'int64',\n",
        "        'high_debt_indicator': 'int64',\n",
        "        'industrial_indicator': 'int64'\n",
        "    }\n",
        "\n",
        "    # Get the set of actual columns from the input DataFrame.\n",
        "    actual_columns = set(df_validated.columns)\n",
        "\n",
        "    # Get the set of expected columns from the schema definition.\n",
        "    expected_columns = set(expected_schema.keys())\n",
        "\n",
        "    # Find any columns that are missing from the DataFrame.\n",
        "    missing_columns = expected_columns - actual_columns\n",
        "\n",
        "    # Find any columns in the DataFrame that are not in the expected schema.\n",
        "    extra_columns = actual_columns - expected_columns\n",
        "\n",
        "    # Check for schema violations.\n",
        "    if missing_columns or extra_columns:\n",
        "        # Construct a detailed error message if the schema does not match.\n",
        "        error_message = \"DataFrame schema validation failed. \"\n",
        "        if missing_columns:\n",
        "            error_message += f\"Missing columns: {sorted(list(missing_columns))}. \"\n",
        "        if extra_columns:\n",
        "            error_message += f\"Extra columns found: {sorted(list(extra_columns))}. \"\n",
        "        # Raise a ValueError with the specific schema issues.\n",
        "        raise ValueError(error_message)\n",
        "\n",
        "    # Validate data types for each column and attempt coercion if necessary.\n",
        "    dtype_errors = {}\n",
        "    for col, expected_dtype in expected_schema.items():\n",
        "        # Check if the actual data type matches the expected data type.\n",
        "        if not pd.api.types.is_dtype_equal(df_validated[col].dtype, expected_dtype):\n",
        "            try:\n",
        "                # Attempt to coerce the column to the correct data type.\n",
        "                if 'int' in expected_dtype:\n",
        "                    # Use pd.to_numeric for robust integer conversion.\n",
        "                    df_validated[col] = pd.to_numeric(df_validated[col], downcast='integer')\n",
        "                else:\n",
        "                    # Use astype for other conversions (float, object).\n",
        "                    df_validated[col] = df_validated[col].astype(expected_dtype)\n",
        "            except (ValueError, TypeError) as e:\n",
        "                # If coercion fails, record the error.\n",
        "                dtype_errors[col] = {\n",
        "                    \"expected\": expected_dtype,\n",
        "                    \"actual\": str(df_validated[col].dtype),\n",
        "                    \"error\": str(e)\n",
        "                }\n",
        "\n",
        "    # If any data type coercion failed, raise an error.\n",
        "    if dtype_errors:\n",
        "        # Raise a ValueError with details about the failed type conversions.\n",
        "        raise ValueError(f\"DataFrame dtype validation failed: {dtype_errors}\")\n",
        "\n",
        "    # Record the successful schema validation in the report.\n",
        "    validation_report['schema_validation'] = {\n",
        "        'status': True,\n",
        "        'details': 'All required columns are present with correct data types.'\n",
        "    }\n",
        "\n",
        "    # =========================================================================\n",
        "    # 2. Panel Dimension Validation (Task 1, Step 2)\n",
        "    # =========================================================================\n",
        "    # Get the number of unique countries (N).\n",
        "    num_countries = df_validated['country_iso'].nunique()\n",
        "\n",
        "    # Get the number of unique years (T).\n",
        "    num_years = df_validated['year'].nunique()\n",
        "\n",
        "    # Get the counts of observations for each country.\n",
        "    obs_per_country = df_validated.groupby('country_iso').size()\n",
        "\n",
        "    # Check if the panel is balanced (each country has the same number of observations).\n",
        "    is_balanced = obs_per_country.nunique() == 1\n",
        "\n",
        "    # Validate against the paper's specific dimensions (N=52, T=33).\n",
        "    if num_countries != 52 or num_years != 33 or not is_balanced:\n",
        "        # Construct a detailed error message for dimension mismatches.\n",
        "        error_message = \"Panel dimension validation failed. \"\n",
        "        if num_countries != 52:\n",
        "            error_message += f\"Expected 52 countries, but found {num_countries}. \"\n",
        "        if num_years != 33:\n",
        "            error_message += f\"Expected 33 years, but found {num_years}. \"\n",
        "        if not is_balanced:\n",
        "            # Identify countries that do not have the expected 33 observations.\n",
        "            unbalanced_countries = obs_per_country[obs_per_country != 33].to_dict()\n",
        "            error_message += f\"Panel is not balanced. Countries with incorrect observation counts: {unbalanced_countries}.\"\n",
        "        # Raise a ValueError with the specific dimension issues.\n",
        "        raise ValueError(error_message)\n",
        "\n",
        "    # Record the successful dimension validation in the report.\n",
        "    validation_report['dimension_validation'] = {\n",
        "        'status': True,\n",
        "        'details': f'Panel is balanced with N={num_countries} and T={num_years}.'\n",
        "    }\n",
        "\n",
        "    # =========================================================================\n",
        "    # 3. Country Code Consistency Validation (Task 1, Step 3)\n",
        "    # =========================================================================\n",
        "    # Extract all unique country codes from the subsample definitions in the parameters.\n",
        "    all_param_countries: Set[str] = set()\n",
        "    # Iterate through the subsample definitions to collect all specified country codes.\n",
        "    for group_name, country_list in analysis_parameters['subsample_definitions'].items():\n",
        "        # Add the countries from the current group to the master set.\n",
        "        all_param_countries.update(country_list)\n",
        "\n",
        "    # Get the set of unique country codes present in the DataFrame.\n",
        "    data_countries = set(df_validated['country_iso'].unique())\n",
        "\n",
        "    # Find country codes in the data that are not defined in the parameters.\n",
        "    unrecognized_countries = data_countries - all_param_countries\n",
        "\n",
        "    # Find country codes defined in the parameters that are not present in the data.\n",
        "    missing_from_data = all_param_countries - data_countries\n",
        "\n",
        "    # Check for any inconsistencies.\n",
        "    if unrecognized_countries or missing_from_data:\n",
        "        # Construct a detailed error message for country code mismatches.\n",
        "        error_message = \"Country code consistency validation failed. \"\n",
        "        if unrecognized_countries:\n",
        "            error_message += f\"Unrecognized country codes in data: {sorted(list(unrecognized_countries))}. \"\n",
        "        if missing_from_data:\n",
        "            error_message += f\"Country codes missing from data: {sorted(list(missing_from_data))}. \"\n",
        "        # Raise a ValueError with the specific country code issues.\n",
        "        raise ValueError(error_message)\n",
        "\n",
        "    # Record the successful country code validation in the report.\n",
        "    validation_report['country_code_validation'] = {\n",
        "        'status': True,\n",
        "        'details': 'All country codes in data match subsample definitions.'\n",
        "    }\n",
        "\n",
        "    # =========================================================================\n",
        "    # 4. Parameter Dictionary Validation (Task 1, Step 4)\n",
        "    # =========================================================================\n",
        "    param_errors = []\n",
        "    try:\n",
        "        # Validate algorithmic parameters.\n",
        "        if not isinstance(analysis_parameters['hp_filter_lambda']['value'], int) or analysis_parameters['hp_filter_lambda']['value'] <= 0:\n",
        "            param_errors.append(\"hp_filter_lambda must be a positive integer.\")\n",
        "        if not isinstance(analysis_parameters['dcce_lags']['value'], int) or not (1 <= analysis_parameters['dcce_lags']['value'] <= 5):\n",
        "            param_errors.append(\"dcce_lags must be an integer between 1 and 5.\")\n",
        "\n",
        "        # Validate model specifications.\n",
        "        model_specs = analysis_parameters['model_specifications']\n",
        "        if not isinstance(model_specs, dict):\n",
        "            param_errors.append(\"'model_specifications' must be a dictionary.\")\n",
        "        else:\n",
        "            # For each model, check that all specified regressors exist in the DataFrame.\n",
        "            for model_name, spec in model_specs.items():\n",
        "                invalid_regressors = set(spec['regressors']) - actual_columns\n",
        "                if invalid_regressors:\n",
        "                    param_errors.append(f\"Model '{model_name}' contains invalid regressors: {invalid_regressors}.\")\n",
        "\n",
        "        # Validate subsample definitions.\n",
        "        subsample_defs = analysis_parameters['subsample_definitions']\n",
        "        if not isinstance(subsample_defs, dict):\n",
        "            param_errors.append(\"'subsample_definitions' must be a dictionary.\")\n",
        "        else:\n",
        "            # Check that each subsample definition is a list of strings.\n",
        "            for group_name, country_list in subsample_defs.items():\n",
        "                if not isinstance(country_list, list) or not all(isinstance(c, str) for c in country_list):\n",
        "                    param_errors.append(f\"Subsample '{group_name}' must be a list of strings.\")\n",
        "\n",
        "    except KeyError as e:\n",
        "        # Catch any missing keys in the parameter dictionary.\n",
        "        param_errors.append(f\"Missing key in analysis_parameters: {e}\")\n",
        "\n",
        "    # If any parameter validation errors were found, raise an error.\n",
        "    if param_errors:\n",
        "        # Raise a ValueError with a list of all parameter validation issues.\n",
        "        raise ValueError(f\"Parameter dictionary validation failed: {'; '.join(param_errors)}\")\n",
        "\n",
        "    # Record the successful parameter validation in the report.\n",
        "    validation_report['parameter_validation'] = {\n",
        "        'status': True,\n",
        "        'details': 'Parameter dictionary structure and content are valid.'\n",
        "    }\n",
        "\n",
        "    # Return the validated DataFrame and the comprehensive report.\n",
        "    return df_validated, validation_report\n"
      ],
      "metadata": {
        "id": "0ZzJiL0UD3zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Data Cleansing\n",
        "\n",
        "def clean_and_prepare_data(\n",
        "    validated_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs systematic data cleansing on a validated panel DataFrame.\n",
        "\n",
        "    This function executes a sequence of cleansing and preparation steps to\n",
        "    ensure the data is robust and ready for econometric analysis, adhering to\n",
        "    the specifications of the source research. The steps are:\n",
        "    1.  Missing Value Analysis: Confirms that missing values only exist where\n",
        "        structurally expected (i.e., for lagged variables in the first year).\n",
        "    2.  Outlier Detection and Treatment: Clips continuous variables to within\n",
        "        economically plausible bounds based on observed data from the paper.\n",
        "    3.  Consistency Validation: Verifies the internal consistency of derived\n",
        "        variables like the GFC dummy and time-invariant country indicators.\n",
        "    4.  Range Validation: Ensures that cyclical components from the HP filter\n",
        "        are centered around zero, as per their theoretical properties.\n",
        "\n",
        "    Args:\n",
        "        validated_df (pd.DataFrame):\n",
        "            A DataFrame that has successfully passed the validation checks from\n",
        "            the `validate_inputs` function. It is assumed to have the correct\n",
        "            schema and dimensions.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - A cleansed and prepared copy of the input DataFrame.\n",
        "            - A detailed cleansing report dictionary summarizing the results\n",
        "              and actions taken in each step.\n",
        "\n",
        "    Raises:\n",
        "        ValueError:\n",
        "            If any unrecoverable data quality issue is detected, such as\n",
        "            unexpected missing values or major inconsistencies.\n",
        "        TypeError:\n",
        "            If the input is not a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    # =========================================================================\n",
        "    # 0. Initial Input Type Validation\n",
        "    # =========================================================================\n",
        "    # Ensure the input is a pandas DataFrame.\n",
        "    if not isinstance(validated_df, pd.DataFrame):\n",
        "        # Raise a TypeError if the input is not of the expected type.\n",
        "        raise TypeError(\"Input 'validated_df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # 1. Initialization\n",
        "    # =========================================================================\n",
        "    # Create a deep copy of the DataFrame to ensure the original is not modified.\n",
        "    df_clean = validated_df.copy()\n",
        "\n",
        "    # Initialize the cleansing report dictionary.\n",
        "    cleansing_report: Dict[str, Any] = {}\n",
        "\n",
        "    # =========================================================================\n",
        "    # 2. Missing Value Analysis (Task 2, Step 1)\n",
        "    # =========================================================================\n",
        "    # This step verifies that NaNs only appear where they are structurally expected.\n",
        "    report_missing = {'status': True, 'details': {}}\n",
        "\n",
        "    # Check for any missing values in columns where none are expected.\n",
        "    cols_to_check = df_clean.columns.drop(['s_it_lag1', 'b_it_lag1'])\n",
        "    unexpected_nans = df_clean[cols_to_check].isnull().sum()\n",
        "    unexpected_nans = unexpected_nans[unexpected_nans > 0]\n",
        "\n",
        "    # If unexpected NaNs are found, the validation fails.\n",
        "    if not unexpected_nans.empty:\n",
        "        # Raise a ValueError with details of the unexpected missing data.\n",
        "        raise ValueError(\n",
        "            \"Unexpected missing values found in columns where none are allowed: \"\n",
        "            f\"{unexpected_nans.to_dict()}\"\n",
        "        )\n",
        "\n",
        "    # Verify that NaNs in lagged variables only occur in the first year (1990).\n",
        "    # Create a boolean mask for rows after 1990.\n",
        "    post_1990_mask = df_clean['year'] != 1990\n",
        "    # Check for any nulls in lagged columns after 1990.\n",
        "    nans_in_lags_post_1990 = df_clean.loc[post_1990_mask, ['s_it_lag1', 'b_it_lag1']].isnull().sum()\n",
        "    nans_in_lags_post_1990 = nans_in_lags_post_1990[nans_in_lags_post_1990 > 0]\n",
        "\n",
        "    # If NaNs are found in lagged variables after 1990, raise an error.\n",
        "    if not nans_in_lags_post_1990.empty:\n",
        "        # Raise a ValueError indicating a violation of the panel structure.\n",
        "        raise ValueError(\n",
        "            \"Missing values for lagged variables found after 1990, \"\n",
        "            f\"indicating an unbalanced panel: {nans_in_lags_post_1990.to_dict()}\"\n",
        "        )\n",
        "\n",
        "    # Drop all rows with any remaining NaN values. Given the checks above,\n",
        "    # this should only remove the observations from the year 1990.\n",
        "    initial_rows = len(df_clean)\n",
        "    df_clean.dropna(inplace=True)\n",
        "    final_rows = len(df_clean)\n",
        "\n",
        "    # Update the report with the outcome of the missing value analysis.\n",
        "    report_missing['details']['initial_rows'] = initial_rows\n",
        "    report_missing['details']['final_rows'] = final_rows\n",
        "    report_missing['details']['rows_dropped'] = initial_rows - final_rows\n",
        "    report_missing['details']['message'] = (\n",
        "        \"Validated that NaNs only exist for lagged variables in 1990. \"\n",
        "        \"These rows have been dropped for estimation.\"\n",
        "    )\n",
        "    cleansing_report['missing_value_analysis'] = report_missing\n",
        "\n",
        "    # =========================================================================\n",
        "    # 3. Outlier Detection and Treatment (Task 2, Step 2)\n",
        "    # =========================================================================\n",
        "    # This step clips data to within plausible economic bounds.\n",
        "    report_outlier = {'status': True, 'details': {}}\n",
        "\n",
        "    # Define the clipping bounds based on paper's data and economic reason.\n",
        "    # Using slightly wider bounds than paper's max to avoid clipping valid extremes.\n",
        "    # b_it: Max in paper is 260.1. We use 2.61 as a fraction.\n",
        "    # s_it, a_it: Using +/- 20% and +/- 60% as very extreme but plausible bounds.\n",
        "    bounds = {\n",
        "        's_it': (-0.20, 0.20),\n",
        "        'b_it': (0.0, 2.61), # Debt cannot be negative.\n",
        "        'a_it': (-0.60, 0.60)\n",
        "    }\n",
        "\n",
        "    # Apply clipping and record the number of affected data points.\n",
        "    for col, (lower, upper) in bounds.items():\n",
        "        # Store the original series to compare after clipping.\n",
        "        original_series = df_clean[col].copy()\n",
        "        # Clip the series to the defined lower and upper bounds.\n",
        "        df_clean[col] = df_clean[col].clip(lower=lower, upper=upper)\n",
        "        # Count how many values were changed by the clipping operation.\n",
        "        clipped_count = (df_clean[col] != original_series).sum()\n",
        "        # Record the clipping details in the report.\n",
        "        report_outlier['details'][col] = {\n",
        "            'bounds': (lower, upper),\n",
        "            'clipped_count': clipped_count\n",
        "        }\n",
        "\n",
        "    cleansing_report['outlier_treatment'] = report_outlier\n",
        "\n",
        "    # =========================================================================\n",
        "    # 4. Consistency Validation (Task 2, Step 3)\n",
        "    # =========================================================================\n",
        "    # This step verifies the internal logic of constructed variables.\n",
        "    report_consistency = {'status': True, 'details': {}}\n",
        "\n",
        "    # Verify the GFC dummy variable is correctly specified (1 for year >= 2008).\n",
        "    # Equation: d_gfc_t = 1 if year >= 2008, 0 otherwise\n",
        "    expected_gfc = (df_clean['year'] >= 2008).astype(int)\n",
        "    if not df_clean['d_gfc_t'].equals(expected_gfc):\n",
        "        # Raise an error if the GFC dummy is inconsistent with the 'year' column.\n",
        "        raise ValueError(\"GFC dummy ('d_gfc_t') is not consistent with 'year' column.\")\n",
        "    report_consistency['details']['d_gfc_t'] = \"Consistent with year >= 2008.\"\n",
        "\n",
        "    # Verify that indicator variables are time-invariant for each country.\n",
        "    for indicator in ['high_debt_indicator', 'industrial_indicator']:\n",
        "        # Group by country and count the number of unique values for the indicator.\n",
        "        nunique_per_country = df_clean.groupby('country_iso')[indicator].nunique()\n",
        "        # If any country has more than one unique value, it's an error.\n",
        "        if not nunique_per_country.eq(1).all():\n",
        "            # Identify the inconsistent countries to provide a specific error message.\n",
        "            inconsistent_countries = nunique_per_country[nunique_per_country != 1].index.tolist()\n",
        "            raise ValueError(\n",
        "                f\"Indicator '{indicator}' is not time-invariant for all countries. \"\n",
        "                f\"Inconsistent countries: {inconsistent_countries}\"\n",
        "            )\n",
        "        report_consistency['details'][indicator] = \"Confirmed as time-invariant.\"\n",
        "\n",
        "    cleansing_report['consistency_validation'] = report_consistency\n",
        "\n",
        "    # =========================================================================\n",
        "    # 5. Range Validation (Task 2, Step 4)\n",
        "    # =========================================================================\n",
        "    # This step ensures cyclical components are properly centered.\n",
        "    report_range = {'status': True, 'details': {}}\n",
        "\n",
        "    # Calculate the mean of the cyclical components.\n",
        "    cyclical_means = df_clean[['y_tilde_it', 'g_tilde_it']].mean()\n",
        "\n",
        "    # Check if the means are close to zero (within a small tolerance).\n",
        "    # A non-zero mean would violate the properties of the HP filter.\n",
        "    tolerance = 1e-9\n",
        "    if (cyclical_means.abs() > tolerance).any():\n",
        "        # Raise an error if the cyclical components are not centered around zero.\n",
        "        raise ValueError(\n",
        "            \"Cyclical components are not centered around zero, violating HP filter \"\n",
        "            f\"properties. Mean values: {cyclical_means.to_dict()}\"\n",
        "        )\n",
        "\n",
        "    # Record the successful validation in the report.\n",
        "    report_range['details']['cyclical_component_means'] = cyclical_means.to_dict()\n",
        "    report_range['details']['message'] = \"Cyclical components are correctly centered around zero.\"\n",
        "    cleansing_report['range_validation'] = report_range\n",
        "\n",
        "    # Return the fully cleansed DataFrame and the detailed report.\n",
        "    return df_clean, cleansing_report\n"
      ],
      "metadata": {
        "id": "FytA1bDKFihI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Data Indexing and Sorting\n",
        "\n",
        "def set_panel_structure(\n",
        "    cleansed_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Transforms a cleansed DataFrame into a canonical panel data structure.\n",
        "\n",
        "    This function performs the critical step of setting and sorting a\n",
        "    MultiIndex, which is foundational for all subsequent time-series and\n",
        "    cross-sectional analysis. The process involves:\n",
        "    1.  Setting a two-level MultiIndex using 'country_iso' (level 0) and\n",
        "        'year' (level 1).\n",
        "    2.  Sorting the DataFrame based on this new index to ensure chronological\n",
        "        order within each country.\n",
        "    3.  Validating the final structure to guarantee monotonicity and uniqueness,\n",
        "        which are essential for reliable econometric operations.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df (pd.DataFrame):\n",
        "            A DataFrame that has been successfully validated and cleansed. It is\n",
        "            expected to have 'country_iso' and 'year' as columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A new DataFrame with a sorted ('country_iso', 'year') MultiIndex,\n",
        "            ready for panel data analysis.\n",
        "\n",
        "    Raises:\n",
        "        ValueError:\n",
        "            If the final index is not unique or not monotonically increasing,\n",
        "            indicating a fundamental structural problem in the data.\n",
        "        TypeError:\n",
        "            If the input is not a pandas DataFrame.\n",
        "        KeyError:\n",
        "            If the required 'country_iso' or 'year' columns are not present.\n",
        "    \"\"\"\n",
        "    # =========================================================================\n",
        "    # 0. Initial Input Type Validation\n",
        "    # =========================================================================\n",
        "    # Ensure the input is a pandas DataFrame.\n",
        "    if not isinstance(cleansed_df, pd.DataFrame):\n",
        "        # Raise a TypeError if the input is not of the expected type.\n",
        "        raise TypeError(\"Input 'cleansed_df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # 1. Initialization\n",
        "    # =========================================================================\n",
        "    # Create a copy to ensure the original DataFrame remains unmodified.\n",
        "    panel_df = cleansed_df.copy()\n",
        "\n",
        "    # Check for the presence of required columns for indexing.\n",
        "    required_cols = ['country_iso', 'year']\n",
        "    if not all(col in panel_df.columns for col in required_cols):\n",
        "        # Raise a KeyError if the necessary columns for the index are missing.\n",
        "        raise KeyError(\n",
        "            f\"Input DataFrame must contain {required_cols} for indexing.\"\n",
        "        )\n",
        "\n",
        "    # =========================================================================\n",
        "    # 2. MultiIndex Creation (Task 3, Step 1)\n",
        "    # =========================================================================\n",
        "    # Set 'country_iso' and 'year' as the two-level MultiIndex.\n",
        "    # This is the canonical structure for panel data in pandas.\n",
        "    panel_df.set_index(required_cols, inplace=True)\n",
        "\n",
        "    # =========================================================================\n",
        "    # 3. Panel Sorting (Task 3, Step 2)\n",
        "    # =========================================================================\n",
        "    # Sort the DataFrame by the MultiIndex.\n",
        "    # Level 0 ('country_iso') is sorted first, then Level 1 ('year').\n",
        "    # This ensures that time-series operations within each country are correct.\n",
        "    panel_df.sort_index(inplace=True)\n",
        "\n",
        "    # =========================================================================\n",
        "    # 4. Final Structure Validation (Task 3, Step 3)\n",
        "    # =========================================================================\n",
        "    # This step validates the integrity of the newly created panel structure.\n",
        "\n",
        "    # Check 1: Verify that the index is unique.\n",
        "    # A non-unique index means there are duplicate (country, year) observations.\n",
        "    if not panel_df.index.is_unique:\n",
        "        # Identify and report the duplicate index entries to aid debugging.\n",
        "        duplicates = panel_df.index[panel_df.index.duplicated()].tolist()\n",
        "        raise ValueError(\n",
        "            \"Panel index is not unique. Found duplicate (country, year) \"\n",
        "            f\"pairs: {duplicates}\"\n",
        "        )\n",
        "\n",
        "    # Check 2: Verify that the index is monotonically increasing.\n",
        "    # This confirms that the sorting was successful and the panel is ordered correctly.\n",
        "    if not panel_df.index.is_monotonic_increasing:\n",
        "        # This error indicates a failure in the sorting logic or underlying data.\n",
        "        raise ValueError(\n",
        "            \"Panel index is not monotonically increasing after sorting. \"\n",
        "            \"This indicates a structural data issue.\"\n",
        "        )\n",
        "\n",
        "    # Return the correctly structured and validated panel DataFrame.\n",
        "    return panel_df\n"
      ],
      "metadata": {
        "id": "1ybP0k33GeSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Descriptive Statistics\n",
        "\n",
        "def generate_descriptive_statistics(\n",
        "    panel_df: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Generates a comprehensive set of descriptive statistics and correlations.\n",
        "\n",
        "    This function takes a panel-structured DataFrame and computes key summary\n",
        "    statistics and correlation matrices, providing a foundational overview of\n",
        "    the data. The analysis is performed for the full panel and for specified\n",
        "    subsamples (high/low debt, industrial/emerging).\n",
        "\n",
        "    The function executes three main steps:\n",
        "    1.  Computes detailed summary statistics for the full panel, including\n",
        "        extended percentiles, skewness, and kurtosis.\n",
        "    2.  Generates the same detailed statistics for each relevant subsample,\n",
        "        allowing for comparison of group characteristics. This step also\n",
        "        produces the time-averaged data needed for visualization.\n",
        "    3.  Calculates multiple types of correlation matrices (Pearson, Spearman,\n",
        "        Kendall) to assess variable relationships.\n",
        "\n",
        "    Args:\n",
        "        panel_df (pd.DataFrame):\n",
        "            A fully cleansed and structured DataFrame with a ('country_iso', 'year')\n",
        "            MultiIndex.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A nested dictionary containing all generated statistics. The structure is:\n",
        "            {\n",
        "                'summary_stats': {\n",
        "                    'full_panel': pd.DataFrame,\n",
        "                    'subsamples': {\n",
        "                        'by_debt_level': {'high_debt': df, 'low_debt': df},\n",
        "                        'by_economic_status': {'industrial': df, 'emerging': df}\n",
        "                    }\n",
        "                },\n",
        "                'correlation_matrices': {\n",
        "                    'full_panel': {'pearson': df, 'spearman': df, 'kendall': df}\n",
        "                },\n",
        "                'visualization_data': {\n",
        "                    'full_panel_avg_by_year': pd.DataFrame,\n",
        "                    'subsample_avg_by_year': pd.DataFrame\n",
        "                }\n",
        "            }\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input is not a pandas DataFrame.\n",
        "        KeyError: If the required MultiIndex or indicator columns are missing.\n",
        "    \"\"\"\n",
        "    # =========================================================================\n",
        "    # 0. Initial Input Validation\n",
        "    # =========================================================================\n",
        "    # Ensure the input is a pandas DataFrame.\n",
        "    if not isinstance(panel_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'panel_df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # Verify that the DataFrame has the expected MultiIndex.\n",
        "    if not isinstance(panel_df.index, pd.MultiIndex) or \\\n",
        "       panel_df.index.names != ['country_iso', 'year']:\n",
        "        raise ValueError(\n",
        "            \"Input 'panel_df' must have a ('country_iso', 'year') MultiIndex.\"\n",
        "        )\n",
        "\n",
        "    # Check for required indicator columns for subsample analysis.\n",
        "    required_indicators = ['high_debt_indicator', 'industrial_indicator']\n",
        "    if not all(col in panel_df.columns for col in required_indicators):\n",
        "        raise KeyError(\n",
        "            f\"DataFrame is missing required indicator columns: {required_indicators}\"\n",
        "        )\n",
        "\n",
        "    # =========================================================================\n",
        "    # 1. Initialization\n",
        "    # =========================================================================\n",
        "    # Define the list of variables for which to compute statistics.\n",
        "    # Exclude indicator columns from the main statistical analysis.\n",
        "    vars_to_analyze = panel_df.columns.drop(required_indicators).tolist()\n",
        "\n",
        "    # Define the custom percentiles for a detailed distributional view.\n",
        "    percentiles = [0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99]\n",
        "\n",
        "    # Initialize the main results dictionary.\n",
        "    results: Dict[str, Any] = {\n",
        "        'summary_stats': {'subsamples': {}},\n",
        "        'correlation_matrices': {'full_panel': {}},\n",
        "        'visualization_data': {}\n",
        "    }\n",
        "\n",
        "    # Helper function to compute extended statistics.\n",
        "    def get_extended_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Computes describe() and adds skew and kurtosis.\"\"\"\n",
        "        # Compute standard descriptive statistics with custom percentiles.\n",
        "        stats = df[vars_to_analyze].describe(percentiles=percentiles).transpose()\n",
        "        # Compute and add skewness for each variable.\n",
        "        stats['skew'] = df[vars_to_analyze].skew()\n",
        "        # Compute and add kurtosis for each variable.\n",
        "        stats['kurtosis'] = df[vars_to_analyze].kurt()\n",
        "        return stats\n",
        "\n",
        "    # =========================================================================\n",
        "    # 2. Comprehensive Summary Statistics (Task 4, Step 1)\n",
        "    # =========================================================================\n",
        "    # Compute and store extended statistics for the entire panel.\n",
        "    results['summary_stats']['full_panel'] = get_extended_stats(panel_df)\n",
        "\n",
        "    # =========================================================================\n",
        "    # 3. Subsample Analysis & Visualization Preparation (Task 4, Step 2)\n",
        "    # =========================================================================\n",
        "    # --- Statistics by Debt Level ---\n",
        "    # Group by the high-debt indicator and compute stats for each group.\n",
        "    stats_by_debt = panel_df.groupby('high_debt_indicator').apply(get_extended_stats)\n",
        "    # Store the results in a structured dictionary.\n",
        "    results['summary_stats']['subsamples']['by_debt_level'] = {\n",
        "        'low_debt': stats_by_debt.loc[0],  # high_debt_indicator == 0\n",
        "        'high_debt': stats_by_debt.loc[1]  # high_debt_indicator == 1\n",
        "    }\n",
        "\n",
        "    # --- Statistics by Economic Status ---\n",
        "    # Map the industrial indicator to human-readable names.\n",
        "    panel_df['economic_status'] = panel_df['industrial_indicator'].map({0: 'emerging', 1: 'industrial'})\n",
        "    # Group by the new status column and compute stats.\n",
        "    stats_by_status = panel_df.groupby('economic_status').apply(get_extended_stats)\n",
        "    # Store the results.\n",
        "    results['summary_stats']['subsamples']['by_economic_status'] = {\n",
        "        'emerging': stats_by_status.loc['emerging'],\n",
        "        'industrial': stats_by_status.loc['industrial']\n",
        "    }\n",
        "    # Drop the temporary helper column.\n",
        "    panel_df.drop(columns=['economic_status'], inplace=True)\n",
        "\n",
        "    # --- Prepare Data for Visualization ---\n",
        "    # Calculate cross-sectional averages for each year for the full panel.\n",
        "    results['visualization_data']['full_panel_avg_by_year'] = \\\n",
        "        panel_df.groupby('year')[vars_to_analyze].mean()\n",
        "\n",
        "    # Calculate cross-sectional averages for each year for all subsamples.\n",
        "    results['visualization_data']['subsample_avg_by_year'] = \\\n",
        "        panel_df.groupby(['year', 'high_debt_indicator', 'industrial_indicator'])[vars_to_analyze].mean()\n",
        "\n",
        "    # =========================================================================\n",
        "    # 4. Correlation Analysis (Task 4, Step 3)\n",
        "    # =========================================================================\n",
        "    # Define the correlation methods to be used.\n",
        "    correlation_methods = ['pearson', 'spearman', 'kendall']\n",
        "\n",
        "    # Compute and store the correlation matrix for each method for the full panel.\n",
        "    for method in correlation_methods:\n",
        "        # The .corr() method computes the pairwise correlation of columns.\n",
        "        results['correlation_matrices']['full_panel'][method] = \\\n",
        "            panel_df[vars_to_analyze].corr(method=method)\n",
        "\n",
        "    # Return the nested dictionary containing all results.\n",
        "    return results"
      ],
      "metadata": {
        "id": "oQ3Kb6nNHQw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Diagnostic Tests\n",
        "\n",
        "def _test_slope_homogeneity(\n",
        "    panel_df: pd.DataFrame,\n",
        "    dependent: str,\n",
        "    exog: List[str]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Tests for slope homogeneity using a poolability F-test.\n",
        "\n",
        "    This test evaluates the null hypothesis that all slope coefficients are\n",
        "    identical across all panel entities. A rejection of the null suggests\n",
        "    that coefficients are heterogeneous, justifying the use of mean-group\n",
        "    estimators like DCCEMG. This is conceptually equivalent to the\n",
        "    Blomquist-Westerlund / Swamy test.\n",
        "\n",
        "    H₀: βᵢ = β for all entities i.\n",
        "    H₁: βᵢ are not all equal.\n",
        "\n",
        "    Args:\n",
        "        panel_df (pd.DataFrame): Panel data with a MultiIndex.\n",
        "        dependent (str): The name of the dependent variable.\n",
        "        exog (List[str]): A list of exogenous variable names.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary with the test statistic, p-value, and\n",
        "                        interpretation.\n",
        "    \"\"\"\n",
        "    # Add a constant to the exogenous variables for the regression.\n",
        "    X = sm.add_constant(panel_df[exog])\n",
        "    # Define the dependent variable.\n",
        "    y = panel_df[dependent]\n",
        "\n",
        "    # Fit a PanelOLS model with entity effects to test for poolability.\n",
        "    # The F-test for poolability is a standard output of this model.\n",
        "    mod = PanelOLS(y, X, entity_effects=True)\n",
        "    result = mod.fit(cov_type='clustered', cluster_entity=True)\n",
        "\n",
        "    # Extract the F-statistic for poolability.\n",
        "    f_stat = result.f_statistic_robust.stat\n",
        "    # Extract the corresponding p-value.\n",
        "    p_value = result.f_statistic_robust.pval\n",
        "\n",
        "    # Interpret the result based on a 5% significance level.\n",
        "    interpretation = (\n",
        "        \"Reject H₀: Evidence of slope heterogeneity.\" if p_value < 0.05\n",
        "        else \"Fail to reject H₀: No evidence of slope heterogeneity.\"\n",
        "    )\n",
        "\n",
        "    # Return the results in a structured dictionary.\n",
        "    return {\n",
        "        'test_statistic': f_stat,\n",
        "        'p_value': p_value,\n",
        "        'interpretation': interpretation\n",
        "    }\n",
        "\n",
        "def _test_pesaran_cd(\n",
        "    residuals: pd.Series,\n",
        "    n_entities: int,\n",
        "    n_time: int\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Implements the Pesaran (2004) CD test for cross-sectional dependence.\n",
        "\n",
        "    This test checks for contemporaneous correlation across entities in the\n",
        "    panel. The null hypothesis is cross-sectional independence.\n",
        "\n",
        "    H₀: E(εᵢₜεⱼₜ) = 0 for i ≠ j.\n",
        "    H₁: E(εᵢₜεⱼₜ) ≠ 0 for at least one pair i ≠ j.\n",
        "\n",
        "    The test statistic is calculated as:\n",
        "    CD = sqrt(2T / (N(N-1))) * (Σ_{i=1}^{N-1} Σ_{j=i+1}^{N} ρ̂_{ij})\n",
        "\n",
        "    Args:\n",
        "        residuals (pd.Series): A series of residuals from a panel regression.\n",
        "        n_entities (int): The number of cross-sectional units (N).\n",
        "        n_time (int): The number of time periods (T).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary with the test statistic, p-value, and\n",
        "                        interpretation.\n",
        "    \"\"\"\n",
        "    # Reshape the residuals from a long series to a T x N wide matrix.\n",
        "    # This format is required for calculating the correlation matrix.\n",
        "    resid_matrix = residuals.unstack(level='country_iso')\n",
        "\n",
        "    # Calculate the N x N correlation matrix of the residuals.\n",
        "    # numpy.corrcoef is highly optimized for this task.\n",
        "    corr_matrix = np.corrcoef(resid_matrix.values, rowvar=False)\n",
        "\n",
        "    # To get the sum of off-diagonal elements, we sum all elements and\n",
        "    # subtract the trace (which is N, as corr(i,i)=1).\n",
        "    # We then divide by 2 because the matrix is symmetric.\n",
        "    sum_off_diagonal_corr = (np.sum(corr_matrix) - n_entities) / 2\n",
        "\n",
        "    # Calculate the scaling factor from the CD formula.\n",
        "    # Equation: sqrt(2T / (N(N-1)))\n",
        "    scale_factor = np.sqrt((2 * n_time) / (n_entities * (n_entities - 1)))\n",
        "\n",
        "    # Calculate the final CD test statistic.\n",
        "    cd_statistic = scale_factor * sum_off_diagonal_corr\n",
        "\n",
        "    # The CD statistic is asymptotically distributed as N(0,1).\n",
        "    # Calculate the two-tailed p-value using the standard normal distribution.\n",
        "    p_value = 2 * norm.sf(np.abs(cd_statistic))\n",
        "\n",
        "    # Interpret the result based on a 5% significance level.\n",
        "    interpretation = (\n",
        "        \"Reject H₀: Evidence of cross-sectional dependence.\" if p_value < 0.05\n",
        "        else \"Fail to reject H₀: No evidence of cross-sectional independence.\"\n",
        "    )\n",
        "\n",
        "    # Return the results in a structured dictionary.\n",
        "    return {\n",
        "        'test_statistic': cd_statistic,\n",
        "        'p_value': p_value,\n",
        "        'interpretation': interpretation\n",
        "    }\n",
        "\n",
        "def _test_cips_unit_root(\n",
        "    panel_df: pd.DataFrame,\n",
        "    variables: List[str]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs the CIPS panel unit root test on specified variables.\n",
        "\n",
        "    The CIPS (Cross-sectionally Augmented Im-Pesaran-Shin) test is designed\n",
        "    for panels with cross-sectional dependence. The null hypothesis is that\n",
        "    all series have a unit root.\n",
        "\n",
        "    H₀: All series contain a unit root.\n",
        "    H₁: At least one series is stationary.\n",
        "\n",
        "    Args:\n",
        "        panel_df (pd.DataFrame): Panel data with a MultiIndex.\n",
        "        variables (List[str]): A list of variables to test for unit roots.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A nested dictionary with test results for each variable.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    # Iterate through each variable specified for testing.\n",
        "    for var in variables:\n",
        "        # Use PanelAR from linearmodels, a specialized tool for this test.\n",
        "        # 'trend': Include a trend in the test regression.\n",
        "        # 'lags': Use 'bic' for automatic, data-driven lag length selection.\n",
        "        test = PanelAR(panel_df[var], lags='bic', trend='ct')\n",
        "\n",
        "        # The result object contains the CIPS statistic and p-value.\n",
        "        cips_result = test.cips()\n",
        "\n",
        "        # Interpret the result based on a 5% significance level.\n",
        "        interpretation = (\n",
        "            \"Reject H₀: Evidence of stationarity for some series.\" if cips_result.pvalue < 0.05\n",
        "            else \"Fail to reject H₀: Evidence of non-stationarity (unit root).\"\n",
        "        )\n",
        "\n",
        "        # Store the results for the current variable.\n",
        "        results[var] = {\n",
        "            'test_statistic': cips_result.stat,\n",
        "            'p_value': cips_result.pvalue,\n",
        "            'interpretation': interpretation\n",
        "        }\n",
        "    return results\n",
        "\n",
        "def run_diagnostic_tests(\n",
        "    panel_df: pd.DataFrame,\n",
        "    analysis_parameters: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes a suite of essential diagnostic tests for panel data analysis.\n",
        "\n",
        "    This function serves as a master orchestrator for conducting key diagnostic\n",
        "    checks that inform the choice of an appropriate econometric model. It tests\n",
        "    for slope homogeneity, cross-sectional dependence, and unit roots.\n",
        "\n",
        "    Args:\n",
        "        panel_df (pd.DataFrame):\n",
        "            A fully cleansed and structured DataFrame with a ('country_iso', 'year')\n",
        "            MultiIndex.\n",
        "        analysis_parameters (Dict[str, Any]):\n",
        "            The dictionary of analysis parameters, used to specify the model\n",
        "            for generating residuals.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A nested dictionary containing the results of all diagnostic tests.\n",
        "    \"\"\"\n",
        "    # =========================================================================\n",
        "    # 0. Initial Input Validation\n",
        "    # =========================================================================\n",
        "    if not isinstance(panel_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'panel_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(analysis_parameters, dict):\n",
        "        raise TypeError(\"Input 'analysis_parameters' must be a dictionary.\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # 1. Setup\n",
        "    # =========================================================================\n",
        "    # Define the dependent variable.\n",
        "    dependent_var = 's_it'\n",
        "    # Define the exogenous variables for the full model (col_2) to get residuals.\n",
        "    exog_vars = analysis_parameters['model_specifications']['col_2']['regressors']\n",
        "\n",
        "    # Get panel dimensions.\n",
        "    N = panel_df.index.get_level_values('country_iso').nunique()\n",
        "    T = panel_df.index.get_level_values('year').nunique()\n",
        "\n",
        "    # Initialize the final results dictionary.\n",
        "    diagnostic_results: Dict[str, Any] = {}\n",
        "\n",
        "    # =========================================================================\n",
        "    # 2. Run Tests\n",
        "    # =========================================================================\n",
        "    # --- Task 5, Step 1: Slope Homogeneity Test ---\n",
        "    diagnostic_results['slope_homogeneity_test'] = _test_slope_homogeneity(\n",
        "        panel_df, dependent_var, exog_vars\n",
        "    )\n",
        "\n",
        "    # --- Task 5, Step 2: Pesaran CD Test ---\n",
        "    # First, get residuals from a Pooled OLS regression.\n",
        "    X_pooled = sm.add_constant(panel_df[exog_vars])\n",
        "    y_pooled = panel_df[dependent_var]\n",
        "    pooled_ols_result = sm.OLS(y_pooled, X_pooled).fit()\n",
        "    residuals = pooled_ols_result.resid\n",
        "\n",
        "    # Run the CD test using the calculated residuals.\n",
        "    diagnostic_results['pesaran_cd_test'] = _test_pesaran_cd(residuals, N, T)\n",
        "\n",
        "    # --- Task 5, Step 3: Fan-Liao-Yao CD+ Test ---\n",
        "    # As reasoned, this highly specialized test is not implemented.\n",
        "    # We note its theoretical importance.\n",
        "    diagnostic_results['fan_liao_yao_cd_plus_test'] = {\n",
        "        'test_statistic': None,\n",
        "        'p_value': None,\n",
        "        'interpretation': \"Not implemented. The paper notes this test has \"\n",
        "                          \"superior power properties to the standard CD test.\"\n",
        "    }\n",
        "\n",
        "    # --- Task 5, Steps 4 & 5: CADF / CIPS Unit Root Test ---\n",
        "    # Test the key variables for unit roots.\n",
        "    vars_to_test_unit_root = ['s_it', 'b_it_lag1']\n",
        "    diagnostic_results['cips_unit_root_test'] = _test_cips_unit_root(\n",
        "        panel_df, vars_to_test_unit_root\n",
        "    )\n",
        "\n",
        "    return diagnostic_results\n"
      ],
      "metadata": {
        "id": "nARrg6A5Hsqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: DCCEMG Model Estimation\n",
        "\n",
        "def _estimate_single_dccemg_model(\n",
        "    subsample_df: pd.DataFrame,\n",
        "    dependent: str,\n",
        "    exog: List[str],\n",
        "    dcce_lags: int\n",
        ") -> Dict[str, pd.Series]:\n",
        "    \"\"\"\n",
        "    Estimates a single DCCE Mean Group model for a given subsample of data.\n",
        "\n",
        "    This function implements the core DCCE Mean Group algorithm from scratch:\n",
        "    1.  Computes cross-sectional averages of all model variables.\n",
        "    2.  Generates specified lags of these cross-sectional averages.\n",
        "    3.  For each country, runs an OLS regression of the dependent variable on\n",
        "        its own regressors and the common correlated effects proxies (the\n",
        "        cross-sectional averages and their lags).\n",
        "    4.  Computes the Mean Group coefficients by averaging the individual\n",
        "        country coefficients.\n",
        "    5.  Calculates the standard errors, t-statistics, and p-values for the\n",
        "        Mean Group estimates.\n",
        "\n",
        "    Args:\n",
        "        subsample_df (pd.DataFrame):\n",
        "            A DataFrame for a specific subsample (e.g., high-debt countries)\n",
        "            with a ('country_iso', 'year') MultiIndex.\n",
        "        dependent (str): The name of the dependent variable.\n",
        "        exog (List[str]): A list of the primary exogenous variables.\n",
        "        dcce_lags (int): The number of lags of the cross-sectional averages\n",
        "                         to include as regressors.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.Series]:\n",
        "            A dictionary containing the estimation results, including Series for\n",
        "            'coefficients', 'std_errors', 't_stats', and 'p_values'.\n",
        "    \"\"\"\n",
        "    # =========================================================================\n",
        "    # 1. Prepare Data for Estimation\n",
        "    # =========================================================================\n",
        "    # Define all variables involved in the model (dependent + exogenous).\n",
        "    model_vars = [dependent] + exog\n",
        "\n",
        "    # --- Step 1: Compute Cross-Sectional Averages ---\n",
        "    # These averages will serve as proxies for the unobserved common factors.\n",
        "    # They are computed ONLY over the provided subsample to prevent data leakage.\n",
        "    # The result is broadcast back to the original df shape.\n",
        "    cs_averages = subsample_df[model_vars].groupby(level='year').transform('mean')\n",
        "    cs_averages.columns = [f'{col}_cs_avg' for col in model_vars]\n",
        "\n",
        "    # --- Step 2: Generate Lags of Cross-Sectional Averages ---\n",
        "    # These capture the dynamics of the common factors.\n",
        "    all_cs_avg_vars = []\n",
        "    for p in range(dcce_lags + 1):\n",
        "        # For each lag p from 0 to dcce_lags...\n",
        "        lagged_cs_avg = cs_averages.groupby(level='country_iso').shift(p)\n",
        "        # Rename columns to reflect the lag.\n",
        "        lagged_cs_avg.columns = [f'{col}_lag{p}' for col in cs_averages.columns]\n",
        "        # Add the lagged variables to the list.\n",
        "        all_cs_avg_vars.append(lagged_cs_avg)\n",
        "\n",
        "    # --- Step 3: Assemble the Full Regression DataFrame ---\n",
        "    # Concatenate the original data with all generated CCE proxies.\n",
        "    regression_df = pd.concat([subsample_df] + all_cs_avg_vars, axis=1)\n",
        "\n",
        "    # Drop rows with NaNs created by the lagging process. This ensures all\n",
        "    # country-level regressions are run on the same time period.\n",
        "    regression_df.dropna(inplace=True)\n",
        "\n",
        "    # Define the final list of CCE proxy variable names.\n",
        "    cce_proxy_vars = [col for df in all_cs_avg_vars for col in df.columns]\n",
        "\n",
        "    # =========================================================================\n",
        "    # 2. Run Individual Country Regressions\n",
        "    # =========================================================================\n",
        "    # Get the unique list of countries in this subsample.\n",
        "    countries = regression_df.index.get_level_values('country_iso').unique()\n",
        "\n",
        "    # Store the results from each country's regression.\n",
        "    country_coeffs_list = []\n",
        "\n",
        "    for country in countries:\n",
        "        # Select the data for the current country.\n",
        "        country_df = regression_df.loc[country]\n",
        "\n",
        "        # Define the full set of regressors for this country's OLS.\n",
        "        # This includes its own exog vars and all CCE proxies.\n",
        "        X_vars = exog + cce_proxy_vars\n",
        "\n",
        "        # Prepare the dependent (y) and independent (X) variables.\n",
        "        y = country_df[dependent]\n",
        "        # Add a constant (intercept) to the regression.\n",
        "        X = sm.add_constant(country_df[X_vars])\n",
        "\n",
        "        try:\n",
        "            # Run the OLS regression for the current country.\n",
        "            model = sm.OLS(y, X).fit()\n",
        "            # Store the coefficients, including the constant.\n",
        "            coeffs = model.params\n",
        "            coeffs['country'] = country\n",
        "            country_coeffs_list.append(coeffs)\n",
        "        except np.linalg.LinAlgError:\n",
        "            # If the regression fails (e.g., due to multicollinearity),\n",
        "            # skip this country and print a warning.\n",
        "            print(f\"Warning: OLS regression failed for country {country}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "    # Convert the list of coefficient series into a DataFrame.\n",
        "    # Each row is a country, each column is a coefficient.\n",
        "    all_coeffs_df = pd.DataFrame(country_coeffs_list).set_index('country')\n",
        "\n",
        "    # Keep only the coefficients of the primary exogenous variables and the constant.\n",
        "    # The coefficients on the CCE proxies are not of direct interest.\n",
        "    primary_coeffs_df = all_coeffs_df[['const'] + exog]\n",
        "\n",
        "    # =========================================================================\n",
        "    # 3. Compute Mean Group Estimates\n",
        "    # =========================================================================\n",
        "    # The number of countries with successful regressions.\n",
        "    N_successful = len(primary_coeffs_df)\n",
        "\n",
        "    # --- Step 4: Compute Mean Group Coefficients ---\n",
        "    # The MG estimate is the simple average of individual country coefficients.\n",
        "    # Equation: β̂_MG = (1/Nₛ) * Σᵢ∈ₛ β̂ᵢ\n",
        "    mg_coeffs = primary_coeffs_df.mean()\n",
        "\n",
        "    # --- Step 5: Compute Mean Group Standard Errors ---\n",
        "    # The SE is the standard deviation of the coefficients divided by sqrt(N).\n",
        "    # Equation: SE(β̂_MG) = sqrt[ (1/(Nₛ(Nₛ-1))) * Σᵢ∈ₛ (β̂ᵢ - β̂_MG)² ]\n",
        "    mg_std_errors = primary_coeffs_df.std() / np.sqrt(N_successful)\n",
        "\n",
        "    # --- Compute T-Statistics and P-Values ---\n",
        "    # The t-statistic is the coefficient divided by its standard error.\n",
        "    mg_t_stats = mg_coeffs / mg_std_errors\n",
        "\n",
        "    # The p-value is calculated from the t-distribution with N-1 degrees of freedom.\n",
        "    mg_p_values = t_dist.sf(np.abs(mg_t_stats), df=N_successful - 1) * 2\n",
        "\n",
        "    # Return all results in a structured dictionary.\n",
        "    return {\n",
        "        'coefficients': mg_coeffs,\n",
        "        'std_errors': mg_std_errors,\n",
        "        't_stats': mg_t_stats,\n",
        "        'p_values': mg_p_values,\n",
        "        'n_obs_successful': N_successful\n",
        "    }\n",
        "\n",
        "def run_dccemg_estimation(\n",
        "    panel_df: pd.DataFrame,\n",
        "    analysis_parameters: Dict[str, Any]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the DCCE Mean Group estimation for all model specifications.\n",
        "\n",
        "    This function iterates through each of the 10 model specifications defined\n",
        "    in the analysis parameters. For each specification, it identifies the\n",
        "    correct subsample of countries and calls the core estimation function\n",
        "    to perform the DCCE Mean Group analysis.\n",
        "\n",
        "    Args:\n",
        "        panel_df (pd.DataFrame):\n",
        "            A fully cleansed and structured DataFrame with a ('country_iso', 'year')\n",
        "            MultiIndex.\n",
        "        analysis_parameters (Dict[str, Any]):\n",
        "            The dictionary of analysis parameters, containing model specifications\n",
        "            and subsample definitions.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]:\n",
        "            A nested dictionary where keys are model names (e.g., 'col_1') and\n",
        "            values are dictionaries containing the full estimation results for\n",
        "            that model.\n",
        "    \"\"\"\n",
        "    # Initialize the final results dictionary.\n",
        "    all_model_results: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "    # Get the dependent variable name.\n",
        "    dependent_var = 's_it'\n",
        "    # Get the number of DCCE lags from parameters.\n",
        "    dcce_lags = analysis_parameters['dcce_lags']['value']\n",
        "\n",
        "    # Get the subsample definitions.\n",
        "    subsample_defs = analysis_parameters['subsample_definitions']\n",
        "\n",
        "    # Loop through all 10 model specifications from the paper.\n",
        "    for model_name, spec in analysis_parameters['model_specifications'].items():\n",
        "        print(f\"Estimating DCCEMG for model: {model_name} ({spec['description']})...\")\n",
        "\n",
        "        # Determine the correct subsample for the current model.\n",
        "        if 'High-Debt' in spec['description']:\n",
        "            country_list = subsample_defs['high_debt']\n",
        "        elif 'Low-Debt' in spec['description']:\n",
        "            country_list = subsample_defs['low_debt']\n",
        "        elif 'Industrial' in spec['description']:\n",
        "            country_list = subsample_defs['industrial']\n",
        "        elif 'Emerging' in spec['description']:\n",
        "            country_list = subsample_defs['emerging']\n",
        "        else: # Aggregate Panel\n",
        "            country_list = panel_df.index.get_level_values('country_iso').unique().tolist()\n",
        "\n",
        "        # Create the subsample DataFrame by slicing the main panel.\n",
        "        subsample_df = panel_df.loc[panel_df.index.get_level_values('country_iso').isin(country_list)]\n",
        "\n",
        "        # Call the core estimation function for this model and subsample.\n",
        "        model_result = _estimate_single_dccemg_model(\n",
        "            subsample_df=subsample_df,\n",
        "            dependent=dependent_var,\n",
        "            exog=spec['regressors'],\n",
        "            dcce_lags=dcce_lags\n",
        "        )\n",
        "\n",
        "        # Store the results for the current model.\n",
        "        all_model_results[model_name] = model_result\n",
        "\n",
        "    return all_model_results\n"
      ],
      "metadata": {
        "id": "5hhKJn8EIdbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Long-run Effects Calculation\n",
        "\n",
        "def _estimate_single_dccemg_model(\n",
        "    subsample_df: pd.DataFrame,\n",
        "    dependent: str,\n",
        "    exog: List[str],\n",
        "    dcce_lags: int\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Estimates a single DCCE Mean Group model for a given subsample of data.\n",
        "\n",
        "    This function implements the core DCCE Mean Group algorithm from scratch:\n",
        "    1.  Computes cross-sectional averages of all model variables for the subsample.\n",
        "    2.  Generates specified lags of these cross-sectional averages.\n",
        "    3.  For each country, runs an OLS regression of the dependent variable on\n",
        "        its own regressors and the common correlated effects proxies.\n",
        "    4.  Computes the Mean Group coefficients by averaging the individual\n",
        "        country coefficients.\n",
        "    5.  Calculates the standard errors, t-statistics, and p-values for the\n",
        "        Mean Group estimates.\n",
        "    6.  Returns all results, including the DataFrame of individual coefficients\n",
        "        needed for subsequent long-run effect calculations.\n",
        "\n",
        "    Args:\n",
        "        subsample_df (pd.DataFrame):\n",
        "            A DataFrame for a specific subsample (e.g., high-debt countries)\n",
        "            with a ('country_iso', 'year') MultiIndex.\n",
        "        dependent (str): The name of the dependent variable.\n",
        "        exog (List[str]): A list of the primary exogenous variables.\n",
        "        dcce_lags (int): The number of lags of the cross-sectional averages\n",
        "                         to include as regressors.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A dictionary containing estimation results, including Series for\n",
        "            'coefficients', 'std_errors', 't_stats', 'p_values', the number\n",
        "            of successful observations, and the DataFrame of individual coeffs.\n",
        "    \"\"\"\n",
        "    # =========================================================================\n",
        "    # 1. Prepare Data for Estimation\n",
        "    # =========================================================================\n",
        "    # Define all variables involved in the model (dependent + exogenous).\n",
        "    model_vars = [dependent] + exog\n",
        "\n",
        "    # --- Step 1: Compute Cross-Sectional Averages ---\n",
        "    # These averages serve as proxies for unobserved common factors.\n",
        "    # They are computed ONLY over the provided subsample to prevent data leakage.\n",
        "    # groupby(level='year') groups all countries for a given year.\n",
        "    # .transform('mean') computes the mean and broadcasts it to the original shape.\n",
        "    cs_averages = subsample_df[model_vars].groupby(level='year').transform('mean')\n",
        "    # Rename columns to distinguish them as cross-sectional averages.\n",
        "    cs_averages.columns = [f'{col}_cs_avg' for col in model_vars]\n",
        "\n",
        "    # --- Step 2: Generate Lags of Cross-Sectional Averages ---\n",
        "    # These capture the dynamics of the common factors.\n",
        "    all_cs_avg_vars = []\n",
        "    # Loop from p=0 (contemporaneous) to the specified number of lags.\n",
        "    for p in range(dcce_lags + 1):\n",
        "        # Group by country to ensure lags are calculated within each time series.\n",
        "        # .shift(p) moves the data down by p periods.\n",
        "        lagged_cs_avg = cs_averages.groupby(level='country_iso').shift(p)\n",
        "        # Rename columns to reflect the specific lag.\n",
        "        lagged_cs_avg.columns = [f'{col}_lag{p}' for col in cs_averages.columns]\n",
        "        # Append the DataFrame of lagged variables to a list.\n",
        "        all_cs_avg_vars.append(lagged_cs_avg)\n",
        "\n",
        "    # --- Step 3: Assemble the Full Regression DataFrame ---\n",
        "    # Concatenate the original subsample data with all generated CCE proxies.\n",
        "    regression_df = pd.concat([subsample_df] + all_cs_avg_vars, axis=1)\n",
        "\n",
        "    # Drop rows with any NaNs. This is crucial as the lagging process\n",
        "    # introduces NaNs at the start of each time series. This ensures all\n",
        "    # country-level regressions are run on the exact same time period.\n",
        "    regression_df.dropna(inplace=True)\n",
        "\n",
        "    # Define the final list of CCE proxy variable names to be used as regressors.\n",
        "    cce_proxy_vars = [col for df in all_cs_avg_vars for col in df.columns]\n",
        "\n",
        "    # =========================================================================\n",
        "    # 2. Run Individual Country Regressions\n",
        "    # =========================================================================\n",
        "    # Get the unique list of countries remaining in the estimation sample.\n",
        "    countries = regression_df.index.get_level_values('country_iso').unique()\n",
        "\n",
        "    # This list will store the coefficient Series from each successful regression.\n",
        "    country_coeffs_list = []\n",
        "\n",
        "    # Loop through each country to perform an individual OLS regression.\n",
        "    for country in countries:\n",
        "        # Select the time-series data for the current country using .loc.\n",
        "        country_df = regression_df.loc[country]\n",
        "\n",
        "        # Define the full set of independent variables for this country's OLS.\n",
        "        # This includes its own primary regressors and all CCE proxies.\n",
        "        X_vars = exog + cce_proxy_vars\n",
        "\n",
        "        # Prepare the dependent variable (y) for the regression.\n",
        "        y = country_df[dependent]\n",
        "        # Prepare the independent variables (X) and add a constant (intercept).\n",
        "        X = sm.add_constant(country_df[X_vars])\n",
        "\n",
        "        try:\n",
        "            # Run the OLS regression using statsmodels.\n",
        "            model = sm.OLS(y, X).fit()\n",
        "            # Extract the estimated coefficients.\n",
        "            coeffs = model.params\n",
        "            # Add the country's identifier to the Series.\n",
        "            coeffs['country'] = country\n",
        "            # Append the full coefficient Series to our list.\n",
        "            country_coeffs_list.append(coeffs)\n",
        "        except np.linalg.LinAlgError:\n",
        "            # If OLS fails (e.g., due to perfect multicollinearity), skip the country.\n",
        "            print(f\"Warning: OLS regression failed for country {country}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "    # If no country regressions were successful, return an empty result structure.\n",
        "    if not country_coeffs_list:\n",
        "        return {\n",
        "            'coefficients': pd.Series(dtype=float),\n",
        "            'std_errors': pd.Series(dtype=float),\n",
        "            't_stats': pd.Series(dtype=float),\n",
        "            'p_values': pd.Series(dtype=float),\n",
        "            'n_obs_successful': 0,\n",
        "            'individual_coeffs': pd.DataFrame()\n",
        "        }\n",
        "\n",
        "    # Convert the list of coefficient Series into a single DataFrame.\n",
        "    all_coeffs_df = pd.DataFrame(country_coeffs_list).set_index('country')\n",
        "\n",
        "    # We are only interested in the primary coefficients, not the CCE proxies.\n",
        "    # Select the columns for the constant and the main exogenous variables.\n",
        "    primary_coeffs_df = all_coeffs_df[['const'] + exog]\n",
        "\n",
        "    # =========================================================================\n",
        "    # 3. Compute Mean Group Estimates\n",
        "    # =========================================================================\n",
        "    # The number of countries for which the regression was successful.\n",
        "    N_successful = len(primary_coeffs_df)\n",
        "\n",
        "    # --- Step 4: Compute Mean Group Coefficients ---\n",
        "    # The MG estimate is the simple arithmetic average of individual coefficients.\n",
        "    # Equation: β̂_MG = (1/Nₛ) * Σᵢ∈ₛ β̂ᵢ\n",
        "    mg_coeffs = primary_coeffs_df.mean()\n",
        "\n",
        "    # --- Step 5: Compute Mean Group Standard Errors ---\n",
        "    # The SE of the mean is the sample standard deviation divided by sqrt(N).\n",
        "    # Equation: SE(β̂_MG) = sqrt[ (1/(Nₛ(Nₛ-1))) * Σᵢ∈ₛ (β̂ᵢ - β̂_MG)² ]\n",
        "    mg_std_errors = primary_coeffs_df.std() / np.sqrt(N_successful)\n",
        "\n",
        "    # --- Compute T-Statistics and P-Values ---\n",
        "    # The t-statistic is the coefficient estimate divided by its standard error.\n",
        "    mg_t_stats = mg_coeffs / mg_std_errors\n",
        "\n",
        "    # The p-value is from the t-distribution with N-1 degrees of freedom.\n",
        "    # We multiply by 2 for a two-tailed test.\n",
        "    mg_p_values = t_dist.sf(np.abs(mg_t_stats), df=N_successful - 1) * 2\n",
        "\n",
        "    # Return all results in a structured dictionary, including the individual coeffs.\n",
        "    return {\n",
        "        'coefficients': mg_coeffs,\n",
        "        'std_errors': mg_std_errors,\n",
        "        't_stats': mg_t_stats,\n",
        "        'p_values': mg_p_values,\n",
        "        'n_obs_successful': N_successful,\n",
        "        'individual_coeffs': primary_coeffs_df\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_long_run_effects(\n",
        "    estimation_results: Dict[str, Dict[str, Any]]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Calculates long-run effects and their significance for DCCEMG models.\n",
        "\n",
        "    This function iterates through the results of each estimated model,\n",
        "    calculates the long-run response of the primary surplus to public debt,\n",
        "    and computes its standard error using the Delta Method. The results are\n",
        "    added in-place to the input dictionary.\n",
        "\n",
        "    The long-run response (LRR) is calculated as:\n",
        "    LRR = ρ / (1 - φ)\n",
        "    where ρ is the coefficient on lagged debt and φ is the coefficient on\n",
        "    the lagged primary surplus.\n",
        "\n",
        "    The standard error is calculated via the Delta Method, requiring the\n",
        "    variance and covariance of the ρ and φ estimators.\n",
        "\n",
        "    Args:\n",
        "        estimation_results (Dict[str, Dict[str, Any]]):\n",
        "            The nested dictionary of results from the DCCEMG estimation,\n",
        "            which must include the DataFrame of individual country coefficients.\n",
        "            This dictionary will be modified in-place.\n",
        "    \"\"\"\n",
        "    # =========================================================================\n",
        "    # 0. Initial Input Validation\n",
        "    # =========================================================================\n",
        "    if not isinstance(estimation_results, dict):\n",
        "        raise TypeError(\"Input 'estimation_results' must be a dictionary.\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # 1. Iterate Through Each Model's Results\n",
        "    # =========================================================================\n",
        "    # Loop through each model specification (e.g., 'col_1', 'col_2', etc.).\n",
        "    for model_name, results in estimation_results.items():\n",
        "        # Check if the necessary coefficient data is present.\n",
        "        if 'coefficients' not in results or results['coefficients'].empty:\n",
        "            print(f\"Warning: Skipping long-run effects for {model_name} due to missing coefficients.\")\n",
        "            results['long_run_effects'] = None\n",
        "            continue\n",
        "\n",
        "        # Define the names of the key coefficients for the LRR calculation.\n",
        "        rho_var = 'b_it_lag1'  # Coefficient for lagged debt (ρ)\n",
        "        phi_var = 's_it_lag1'  # Coefficient for lagged surplus (φ)\n",
        "\n",
        "        # Check if both required variables for LRR are in the model's coefficients.\n",
        "        if not (rho_var in results['coefficients'].index and phi_var in results['coefficients'].index):\n",
        "            # If the model is static (no lagged surplus), LRR is not applicable.\n",
        "            results['long_run_effects'] = None\n",
        "            continue\n",
        "\n",
        "        # =====================================================================\n",
        "        # 2. Compute Long-Run Response (LRR) Point Estimate\n",
        "        # =====================================================================\n",
        "        # Extract the Mean Group estimates for rho and phi.\n",
        "        rho_mg = results['coefficients'][rho_var]\n",
        "        phi_mg = results['coefficients'][phi_var]\n",
        "\n",
        "        # Check for the stationarity condition (phi < 1).\n",
        "        # If phi >= 1, the long-run effect is explosive or undefined.\n",
        "        if phi_mg >= 1.0 or np.isclose(phi_mg, 1.0):\n",
        "            lrr_estimate = np.nan\n",
        "            lrr_se = np.nan\n",
        "        else:\n",
        "            # Equation: LRR = ρ / (1 - φ)\n",
        "            lrr_estimate = rho_mg / (1 - phi_mg)\n",
        "\n",
        "            # =================================================================\n",
        "            # 3. Compute LRR Standard Error via Delta Method\n",
        "            # =================================================================\n",
        "            # Retrieve the DataFrame of individual country coefficients.\n",
        "            individual_coeffs = results.get('individual_coeffs')\n",
        "            # Check if the required data for SE calculation is available.\n",
        "            if individual_coeffs is None or individual_coeffs.empty:\n",
        "                 print(f\"Warning: Skipping SE for {model_name}, missing individual coeffs.\")\n",
        "                 lrr_se = np.nan\n",
        "            else:\n",
        "                # Get the number of successful regressions for this model.\n",
        "                N = results['n_obs_successful']\n",
        "\n",
        "                # Compute the (2x2) variance-covariance matrix of the individual coefficients.\n",
        "                cov_matrix_individual = individual_coeffs[[rho_var, phi_var]].cov()\n",
        "\n",
        "                # The covariance matrix of the *mean* is Cov(indiv) / N.\n",
        "                cov_matrix_mean = cov_matrix_individual / N\n",
        "\n",
        "                # Extract the necessary variance and covariance terms.\n",
        "                var_rho = cov_matrix_mean.loc[rho_var, rho_var]\n",
        "                var_phi = cov_matrix_mean.loc[phi_var, phi_var]\n",
        "                cov_rho_phi = cov_matrix_mean.loc[rho_var, phi_var]\n",
        "\n",
        "                # Define the gradient vector (partial derivatives of LRR w.r.t. rho and phi).\n",
        "                # ∂(LRR)/∂(ρ) = 1 / (1 - φ)\n",
        "                grad_rho = 1 / (1 - phi_mg)\n",
        "                # ∂(LRR)/∂(φ) = ρ / (1 - φ)²\n",
        "                grad_phi = rho_mg / ((1 - phi_mg) ** 2)\n",
        "\n",
        "                # Apply the Delta Method formula for the variance of LRR: g' * V * g\n",
        "                # where g is the gradient and V is the covariance matrix of the estimators.\n",
        "                lrr_variance = (\n",
        "                    (grad_rho ** 2 * var_rho) +\n",
        "                    (grad_phi ** 2 * var_phi) +\n",
        "                    (2 * grad_rho * grad_phi * cov_rho_phi)\n",
        "                )\n",
        "\n",
        "                # The standard error is the square root of the variance.\n",
        "                lrr_se = np.sqrt(lrr_variance) if lrr_variance >= 0 else np.nan\n",
        "\n",
        "        # =====================================================================\n",
        "        # 4. Compute T-Statistic and P-Value for LRR\n",
        "        # =====================================================================\n",
        "        # Check if the SE is valid for calculation.\n",
        "        if np.isnan(lrr_estimate) or np.isnan(lrr_se) or lrr_se == 0:\n",
        "            lrr_t_stat = np.nan\n",
        "            lrr_p_value = np.nan\n",
        "        else:\n",
        "            # The t-statistic is the point estimate divided by its standard error.\n",
        "            lrr_t_stat = lrr_estimate / lrr_se\n",
        "            # The p-value is from the t-distribution with N-1 degrees of freedom.\n",
        "            N = results['n_obs_successful']\n",
        "            lrr_p_value = t_dist.sf(np.abs(lrr_t_stat), df=N - 1) * 2\n",
        "\n",
        "        # =====================================================================\n",
        "        # 5. Store Results\n",
        "        # =====================================================================\n",
        "        # Add the calculated long-run effects to the model's results dictionary.\n",
        "        results['long_run_effects'] = {\n",
        "            'point_estimate': lrr_estimate,\n",
        "            'std_error': lrr_se,\n",
        "            't_stat': lrr_t_stat,\n",
        "            'p_value': lrr_p_value\n",
        "        }\n"
      ],
      "metadata": {
        "id": "fqsyYl5nJNAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Statistical Inference\n",
        "\n",
        "def add_significance_indicators(\n",
        "    estimation_results: Dict[str, Dict[str, Any]]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Adds statistical significance indicators (stars) to estimation results.\n",
        "\n",
        "    This function processes a dictionary of model estimation results, adding a\n",
        "    new Series of significance indicators ('*', '**', '***') based on the\n",
        "    p-values of the coefficients and long-run effects. The modification is\n",
        "    done in-place.\n",
        "\n",
        "    Significance levels are defined as:\n",
        "    - '***': p < 0.01\n",
        "    - '**': 0.01 <= p < 0.05\n",
        "    - '*': 0.05 <= p < 0.10\n",
        "    - '': p >= 0.10\n",
        "\n",
        "    Args:\n",
        "        estimation_results (Dict[str, Dict[str, Any]]):\n",
        "            The nested dictionary of results from DCCEMG estimation and\n",
        "            long-run effect calculation. This dictionary will be modified\n",
        "            in-place.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input is not a dictionary.\n",
        "        KeyError: If a model's results are missing the 'p_values' key.\n",
        "    \"\"\"\n",
        "    # =========================================================================\n",
        "    # 0. Initial Input Validation\n",
        "    # =========================================================================\n",
        "    # Ensure the input is a dictionary.\n",
        "    if not isinstance(estimation_results, dict):\n",
        "        raise TypeError(\"Input 'estimation_results' must be a dictionary.\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # 1. Define Significance Bins and Labels\n",
        "    # =========================================================================\n",
        "    # Define the p-value thresholds for significance.\n",
        "    bins = [0, 0.01, 0.05, 0.1, 1.0]\n",
        "    # Define the corresponding star labels for each bin.\n",
        "    labels = ['***', '**', '*', '']\n",
        "\n",
        "    # =========================================================================\n",
        "    # 2. Iterate Through Each Model and Add Indicators\n",
        "    # =========================================================================\n",
        "    # Loop through each model specification in the results dictionary.\n",
        "    for model_name, results in estimation_results.items():\n",
        "        # Check if the p-values Series exists for the main coefficients.\n",
        "        if 'p_values' not in results or results['p_values'].empty:\n",
        "            # If not, skip this model and print a warning.\n",
        "            print(f\"Warning: Skipping significance for {model_name} coefficients due to missing p-values.\")\n",
        "        else:\n",
        "            # --- Step 1: Assign Stars for Main Coefficients ---\n",
        "            # Use pandas.cut to categorize p-values into the defined bins.\n",
        "            # This is a highly efficient, vectorized operation.\n",
        "            # `right=True` means the interval is closed on the right (e.g., (0.01, 0.05]).\n",
        "            # `include_lowest=True` ensures that p-value 0 is included in the first bin.\n",
        "            p_values = results['p_values']\n",
        "            results['significance'] = pd.cut(\n",
        "                p_values,\n",
        "                bins=bins,\n",
        "                labels=labels,\n",
        "                right=True,\n",
        "                include_lowest=True\n",
        "            ).astype(str).replace('nan', '') # Ensure NaNs become empty strings\n",
        "\n",
        "        # --- Step 2: Assign Stars for Long-Run Effects ---\n",
        "        # Check if long-run effects were calculated and have a p-value.\n",
        "        if 'long_run_effects' in results and \\\n",
        "           results['long_run_effects'] and \\\n",
        "           'p_value' in results['long_run_effects']:\n",
        "\n",
        "            # Extract the single p-value for the long-run effect.\n",
        "            lrr_p_value = results['long_run_effects']['p_value']\n",
        "\n",
        "            # Handle the case where the p-value might be NaN.\n",
        "            if pd.isna(lrr_p_value):\n",
        "                lrr_sig = ''\n",
        "            else:\n",
        "                # Apply the same binning logic to the single p-value.\n",
        "                # pd.cut on a scalar returns an array, so we take the first element.\n",
        "                lrr_sig = pd.cut(\n",
        "                    [lrr_p_value],\n",
        "                    bins=bins,\n",
        "                    labels=labels,\n",
        "                    right=True,\n",
        "                    include_lowest=True\n",
        "                )[0]\n",
        "\n",
        "            # Add the significance indicator to the long_run_effects dictionary.\n",
        "            results['long_run_effects']['significance'] = str(lrr_sig).replace('nan', '')\n"
      ],
      "metadata": {
        "id": "KnIls_DLMIbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Results Compilation\n",
        "\n",
        "def compile_results_table(\n",
        "    estimation_results: Dict[str, Dict[str, Any]],\n",
        "    analysis_parameters: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compiles all estimation results into a single, publication-quality table.\n",
        "\n",
        "    This function replicates the format of Table 2 from the source paper. It\n",
        "    takes the nested dictionary of all model results and meticulously formats\n",
        "    and arranges the coefficients, standard errors, significance indicators,\n",
        "    and long-run effects into a final, presentable pandas DataFrame.\n",
        "\n",
        "    The table structure includes:\n",
        "    - Columns for each of the 10 model specifications.\n",
        "    - Interleaved rows for coefficient estimates and their standard errors.\n",
        "    - Significance stars appended to the coefficient estimates.\n",
        "    - A separate section for the calculated long-run response to debt.\n",
        "\n",
        "    Args:\n",
        "        estimation_results (Dict[str, Dict[str, Any]]):\n",
        "            The fully populated dictionary of results from all previous tasks.\n",
        "        analysis_parameters (Dict[str, Any]):\n",
        "            The dictionary of analysis parameters, used to get model descriptions\n",
        "            and the canonical order of variables.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A formatted DataFrame ready for display or export, replicating the\n",
        "            structure and style of a standard academic results table.\n",
        "    \"\"\"\n",
        "    # =========================================================================\n",
        "    # 0. Initial Input Validation\n",
        "    # =========================================================================\n",
        "    if not isinstance(estimation_results, dict):\n",
        "        raise TypeError(\"Input 'estimation_results' must be a dictionary.\")\n",
        "    if not isinstance(analysis_parameters, dict):\n",
        "        raise TypeError(\"Input 'analysis_parameters' must be a dictionary.\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # 1. Setup Table Structure\n",
        "    # =========================================================================\n",
        "    # Define the order and display names of the variables for the table rows.\n",
        "    variable_map = {\n",
        "        's_it_lag1': 'Lagged primary balance-to-GDP ratio',\n",
        "        'b_it_lag1': 'Lagged debt-to-GDP ratio',\n",
        "        'const': 'Constant',\n",
        "        'y_tilde_it': 'Output gap',\n",
        "        'g_tilde_it': 'Spending gap',\n",
        "        'a_it': 'Current account-to-GDP ratio',\n",
        "        'd_gfc_t': '2008 Global Financial Crisis dummy'\n",
        "    }\n",
        "\n",
        "    # Define the order of columns based on the model specifications.\n",
        "    model_order = [f'col_{i}' for i in range(1, 11)]\n",
        "\n",
        "    # Initialize a list to hold the data for each row of the final table.\n",
        "    table_rows = []\n",
        "\n",
        "    # =========================================================================\n",
        "    # 2. Populate Rows for Coefficients and Standard Errors\n",
        "    # =========================================================================\n",
        "    # Iterate through each variable in the predefined order.\n",
        "    for var_code, var_name in variable_map.items():\n",
        "        # Create a dictionary for the coefficient row.\n",
        "        coeff_row = {'Variable': var_name}\n",
        "        # Create a dictionary for the standard error row.\n",
        "        se_row = {'Variable': ' (Standard Error)'}\n",
        "\n",
        "        # Iterate through each model to populate the columns for this variable.\n",
        "        for model_name in model_order:\n",
        "            # Get the results for the current model.\n",
        "            model_res = estimation_results.get(model_name, {})\n",
        "\n",
        "            # Safely get the coefficient, SE, and significance star.\n",
        "            coeff = model_res.get('coefficients', {}).get(var_code, np.nan)\n",
        "            se = model_res.get('std_errors', {}).get(var_code, np.nan)\n",
        "            sig = model_res.get('significance', {}).get(var_code, '')\n",
        "\n",
        "            # Format the coefficient cell if the value is not NaN.\n",
        "            if pd.notna(coeff):\n",
        "                # Format: \"0.123***\"\n",
        "                coeff_row[model_name] = f\"{coeff:.3f}{sig}\"\n",
        "            else:\n",
        "                coeff_row[model_name] = ''\n",
        "\n",
        "            # Format the standard error cell if the value is not NaN.\n",
        "            if pd.notna(se):\n",
        "                # Format: \"(0.0456)\"\n",
        "                se_row[model_name] = f\"({se:.4f})\"\n",
        "            else:\n",
        "                se_row[model_name] = ''\n",
        "\n",
        "        # Append the fully populated rows to the list.\n",
        "        table_rows.append(coeff_row)\n",
        "        table_rows.append(se_row)\n",
        "\n",
        "    # =========================================================================\n",
        "    # 3. Populate Row for Long-Run Effects\n",
        "    # =========================================================================\n",
        "    # Add a separator row for clarity.\n",
        "    table_rows.append({'Variable': ''}) # Empty row as a visual break\n",
        "\n",
        "    # Create dictionaries for the long-run response estimate and its SE.\n",
        "    lrr_coeff_row = {'Variable': 'Long-run response to debt'}\n",
        "    lrr_se_row = {'Variable': ' (Standard Error)'}\n",
        "\n",
        "    # Iterate through each model to populate the LRR rows.\n",
        "    for model_name in model_order:\n",
        "        # Get the long-run effects results for the current model.\n",
        "        lrr_res = estimation_results.get(model_name, {}).get('long_run_effects')\n",
        "\n",
        "        # Check if LRR results exist and are valid.\n",
        "        if lrr_res and pd.notna(lrr_res.get('point_estimate')):\n",
        "            lrr_coeff = lrr_res['point_estimate']\n",
        "            lrr_se = lrr_res['std_error']\n",
        "            lrr_sig = lrr_res.get('significance', '')\n",
        "\n",
        "            # Format the LRR estimate cell.\n",
        "            lrr_coeff_row[model_name] = f\"{lrr_coeff:.3f}{lrr_sig}\"\n",
        "            # Format the LRR standard error cell.\n",
        "            lrr_se_row[model_name] = f\"({lrr_se:.4f})\" if pd.notna(lrr_se) else ''\n",
        "        else:\n",
        "            # Leave cells empty if LRR was not applicable or could not be calculated.\n",
        "            lrr_coeff_row[model_name] = ''\n",
        "            lrr_se_row[model_name] = ''\n",
        "\n",
        "    # Append the LRR rows to the list.\n",
        "    table_rows.append(lrr_coeff_row)\n",
        "    table_rows.append(lrr_se_row)\n",
        "\n",
        "    # =========================================================================\n",
        "    # 4. Add Observation Count Row\n",
        "    # =========================================================================\n",
        "    # Add another separator row.\n",
        "    table_rows.append({'Variable': ''})\n",
        "\n",
        "    # Create a dictionary for the number of observations row.\n",
        "    obs_row = {'Variable': 'Observations (Countries)'}\n",
        "    for model_name in model_order:\n",
        "        # Get the number of successful country regressions for the model.\n",
        "        n_obs = estimation_results.get(model_name, {}).get('n_obs_successful')\n",
        "        obs_row[model_name] = str(n_obs) if pd.notna(n_obs) else ''\n",
        "    table_rows.append(obs_row)\n",
        "\n",
        "    # =========================================================================\n",
        "    # 5. Finalize DataFrame\n",
        "    # =========================================================================\n",
        "    # Create the final DataFrame from the list of row dictionaries.\n",
        "    final_table = pd.DataFrame(table_rows)\n",
        "    # Set the 'Variable' column as the index of the table.\n",
        "    final_table.set_index('Variable', inplace=True)\n",
        "\n",
        "    # Create a hierarchical column header for better readability.\n",
        "    model_descriptions = [\n",
        "        analysis_parameters['model_specifications'][m]['description']\n",
        "        for m in model_order\n",
        "    ]\n",
        "    final_table.columns = pd.MultiIndex.from_tuples(\n",
        "        list(zip(model_descriptions, final_table.columns)),\n",
        "        names=['Model Description', 'Model Column']\n",
        "    )\n",
        "\n",
        "    return final_table\n"
      ],
      "metadata": {
        "id": "MOLgf0RcNzau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline Function\n",
        "\n",
        "def run_fiscal_sustainability_analysis(\n",
        "    raw_df: pd.DataFrame,\n",
        "    analysis_parameters: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the end-to-end fiscal sustainability research pipeline.\n",
        "\n",
        "    This master orchestrator function serves as the main entry point for the\n",
        "    entire analysis. It sequentially calls each modular function, from initial\n",
        "    data validation to final results compilation, ensuring a robust and\n",
        "    replicable workflow.\n",
        "\n",
        "    The pipeline consists of the following stages:\n",
        "    1.  Input Validation: Verifies the schema and dimensions of the raw data.\n",
        "    2.  Data Cleansing: Handles missing values and outliers.\n",
        "    3.  Panel Structuring: Sets the proper MultiIndex for panel analysis.\n",
        "    4.  Descriptive Statistics: Generates summary statistics and correlations.\n",
        "    5.  Diagnostic Testing: Checks for heterogeneity, CSD, and unit roots.\n",
        "    6.  DCCEMG Estimation: Runs the core panel data models.\n",
        "    7.  Long-Run Effects: Calculates the key sustainability metric.\n",
        "    8.  Statistical Inference: Adds significance indicators.\n",
        "    9.  Results Compilation: Creates the final, publication-quality table.\n",
        "\n",
        "    Args:\n",
        "        raw_df (pd.DataFrame):\n",
        "            The raw input panel data in a long format.\n",
        "        analysis_parameters (Dict[str, Any]):\n",
        "            A dictionary containing all algorithmic, model, and subsample\n",
        "            specifications for the analysis.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A comprehensive dictionary containing all reports, intermediate\n",
        "            data, and final results from every stage of the analysis pipeline.\n",
        "\n",
        "    Raises:\n",
        "        Exception: Catches and re-raises any exception from the pipeline's\n",
        "                   stages, adding context about which stage failed.\n",
        "    \"\"\"\n",
        "    # Initialize the master dictionary to store all outputs.\n",
        "    master_results: Dict[str, Any] = {}\n",
        "\n",
        "    try:\n",
        "        # =====================================================================\n",
        "        # STAGE 1: INPUT VALIDATION (Task 1)\n",
        "        # =====================================================================\n",
        "        # Validate the raw DataFrame and parameters.\n",
        "        print(\"Stage 1: Validating inputs...\")\n",
        "        validated_df, validation_report = validate_inputs(raw_df, analysis_parameters)\n",
        "        # Store the validation report.\n",
        "        master_results['validation_report'] = validation_report\n",
        "        print(\"...Validation successful.\")\n",
        "\n",
        "        # =====================================================================\n",
        "        # STAGE 2: DATA CLEANSING (Task 2)\n",
        "        # =====================================================================\n",
        "        # Clean the validated data (handle NaNs, outliers).\n",
        "        print(\"Stage 2: Cleansing data...\")\n",
        "        cleansed_df, cleansing_report = clean_and_prepare_data(validated_df)\n",
        "        # Store the cleansing report.\n",
        "        master_results['cleansing_report'] = cleansing_report\n",
        "        print(\"...Cleansing successful.\")\n",
        "\n",
        "        # =====================================================================\n",
        "        # STAGE 3: PANEL STRUCTURING (Task 3)\n",
        "        # =====================================================================\n",
        "        # Set the MultiIndex to create the canonical panel structure.\n",
        "        print(\"Stage 3: Setting panel structure...\")\n",
        "        panel_df = set_panel_structure(cleansed_df)\n",
        "        # Store the final analysis-ready DataFrame.\n",
        "        master_results['analysis_dataframe'] = panel_df\n",
        "        print(\"...Panel structure set successfully.\")\n",
        "\n",
        "        # =====================================================================\n",
        "        # STAGE 4: DESCRIPTIVE STATISTICS (Task 4)\n",
        "        # =====================================================================\n",
        "        # Generate summary statistics and correlation matrices.\n",
        "        print(\"Stage 4: Generating descriptive statistics...\")\n",
        "        master_results['descriptive_statistics'] = generate_descriptive_statistics(panel_df)\n",
        "        print(\"...Descriptive statistics generated.\")\n",
        "\n",
        "        # =====================================================================\n",
        "        # STAGE 5: DIAGNOSTIC TESTS (Task 5)\n",
        "        # =====================================================================\n",
        "        # Run the suite of essential panel diagnostic tests.\n",
        "        print(\"Stage 5: Running diagnostic tests...\")\n",
        "        master_results['diagnostic_tests'] = run_diagnostic_tests(panel_df, analysis_parameters)\n",
        "        print(\"...Diagnostic tests complete.\")\n",
        "\n",
        "        # =====================================================================\n",
        "        # STAGE 6: DCCEMG ESTIMATION (Task 6)\n",
        "        # =====================================================================\n",
        "        # Run the core DCCE Mean Group estimation for all 10 models.\n",
        "        # This uses the fully remediated estimation function.\n",
        "        print(\"Stage 6: Running DCCEMG estimations for all models...\")\n",
        "        estimation_results = run_dccemg_estimation(panel_df, analysis_parameters)\n",
        "        print(\"...DCCEMG estimations complete.\")\n",
        "\n",
        "        # =====================================================================\n",
        "        # STAGE 7: LONG-RUN EFFECTS (Task 7)\n",
        "        # =====================================================================\n",
        "        # Calculate the long-run response to debt and its standard error.\n",
        "        print(\"Stage 7: Calculating long-run effects...\")\n",
        "        calculate_long_run_effects(estimation_results)\n",
        "        # Store the augmented estimation results.\n",
        "        master_results['estimation_results'] = estimation_results\n",
        "        print(\"...Long-run effects calculated.\")\n",
        "\n",
        "        # =====================================================================\n",
        "        # STAGE 8: STATISTICAL INFERENCE (Task 8)\n",
        "        # =====================================================================\n",
        "        # Add significance stars ('*', '**', '***') to the results.\n",
        "        print(\"Stage 8: Adding significance indicators...\")\n",
        "        add_significance_indicators(estimation_results)\n",
        "        print(\"...Significance indicators added.\")\n",
        "\n",
        "        # =====================================================================\n",
        "        # STAGE 9: RESULTS COMPILATION (Task 9)\n",
        "        # =====================================================================\n",
        "        # Compile all results into a final, publication-quality table.\n",
        "        print(\"Stage 9: Compiling final results table...\")\n",
        "        final_table = compile_results_table(estimation_results, analysis_parameters)\n",
        "        # Store the formatted table.\n",
        "        master_results['final_formatted_table'] = final_table\n",
        "        print(\"...Results table compiled.\")\n",
        "\n",
        "        # Final success message.\n",
        "        print(\"\\nEnd-to-end fiscal sustainability analysis pipeline completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any error from any stage and provide a clear, contextual message.\n",
        "        print(f\"\\nERROR: The analysis pipeline failed at stage: {e.__class__.__name__}\")\n",
        "        print(f\"Error details: {e}\")\n",
        "        # Re-raise the exception to halt execution and allow for debugging.\n",
        "        raise\n",
        "\n",
        "    # Return the master dictionary containing all outputs from the pipeline.\n",
        "    return master_results\n"
      ],
      "metadata": {
        "id": "EJyJ_z9jTByd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Robustness Checks\n",
        "\n",
        "def run_robustness_checks(\n",
        "    raw_df: pd.DataFrame,\n",
        "    analysis_parameters: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs a suite of robustness checks on the main analysis.\n",
        "\n",
        "    This function systematically alters key modeling assumptions and re-runs the\n",
        "    entire end-to-end analysis pipeline to test the stability of the main\n",
        "    findings. It is a critical step in validating the credibility of the\n",
        "    empirical results.\n",
        "\n",
        "    The checks performed are:\n",
        "    1.  Alternative Lag Structures: Re-estimates the DCCE models using\n",
        "        different lag lengths for the common factor proxies (e.g., 2 and 4).\n",
        "    2.  Alternative GFC Dummy Definitions: Re-runs the analysis with different\n",
        "        start and end years for the Global Financial Crisis dummy.\n",
        "    3.  Winsorized Data: Re-runs the analysis on data where extreme outliers\n",
        "        have been clipped at the 1% and 99% percentiles.\n",
        "\n",
        "    Args:\n",
        "        raw_df (pd.DataFrame):\n",
        "            The raw input panel data in a long format.\n",
        "        analysis_parameters (Dict[str, Any]):\n",
        "            The baseline dictionary of analysis parameters.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A nested dictionary containing the final formatted results table\n",
        "            for each robustness check performed.\n",
        "    \"\"\"\n",
        "    # Initialize the master dictionary to store all robustness results.\n",
        "    robustness_results: Dict[str, Any] = {}\n",
        "\n",
        "    # =========================================================================\n",
        "    # 1. Alternative Lag Structures (Task 10, Step 1)\n",
        "    # =========================================================================\n",
        "    print(\"\\n--- Running Robustness Check 1: Alternative Lag Structures ---\")\n",
        "    robustness_results['alternative_lags'] = {}\n",
        "    # Define the alternative lag lengths to test, excluding the baseline.\n",
        "    alternative_lags = [2, 4]\n",
        "    baseline_lags = analysis_parameters['dcce_lags']['value']\n",
        "\n",
        "    for lags in alternative_lags:\n",
        "        # Ensure we don't re-run the baseline specification.\n",
        "        if lags == baseline_lags:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nRunning pipeline with DCCE lags = {lags}...\")\n",
        "        # Create a deep copy of the parameters to avoid modifying the original.\n",
        "        params_copy = copy.deepcopy(analysis_parameters)\n",
        "        # Update the dcce_lags value for this specific run.\n",
        "        params_copy['dcce_lags']['value'] = lags\n",
        "\n",
        "        try:\n",
        "            # Re-run the entire analysis pipeline with the modified parameters.\n",
        "            results = run_fiscal_sustainability_analysis(raw_df, params_copy)\n",
        "            # Store the final formatted table for this lag specification.\n",
        "            robustness_results['alternative_lags'][f'lags_{lags}'] = results['final_formatted_table']\n",
        "            print(f\"...Pipeline with {lags} lags completed successfully.\")\n",
        "        except Exception as e:\n",
        "            # If the pipeline fails for this specification, record the error.\n",
        "            error_msg = f\"Pipeline failed for DCCE lags = {lags}: {e}\"\n",
        "            print(error_msg)\n",
        "            robustness_results['alternative_lags'][f'lags_{lags}'] = error_msg\n",
        "\n",
        "    # =========================================================================\n",
        "    # 2. Alternative GFC Dummy Definitions (Task 10, Step 2)\n",
        "    # =========================================================================\n",
        "    print(\"\\n--- Running Robustness Check 2: Alternative GFC Dummies ---\")\n",
        "    robustness_results['alternative_gfc_dummies'] = {}\n",
        "    # Define alternative start/end years for the GFC period.\n",
        "    alternative_gfc_defs = {\n",
        "        'gfc_2007_onward': (2007, 9999),\n",
        "        'gfc_2007_2009': (2007, 2009)\n",
        "    }\n",
        "\n",
        "    for name, (start_year, end_year) in alternative_gfc_defs.items():\n",
        "        print(f\"\\nRunning pipeline with GFC definition: {name}...\")\n",
        "        # Create a deep copy of the raw DataFrame for modification.\n",
        "        df_copy = raw_df.copy(deep=True)\n",
        "        # Recalculate the 'd_gfc_t' column based on the alternative definition.\n",
        "        df_copy['d_gfc_t'] = (\n",
        "            (df_copy['year'] >= start_year) & (df_copy['year'] <= end_year)\n",
        "        ).astype(int)\n",
        "\n",
        "        try:\n",
        "            # Re-run the entire analysis pipeline with the modified DataFrame.\n",
        "            results = run_fiscal_sustainability_analysis(df_copy, analysis_parameters)\n",
        "            # Store the final formatted table for this GFC definition.\n",
        "            robustness_results['alternative_gfc_dummies'][name] = results['final_formatted_table']\n",
        "            print(f\"...Pipeline with GFC definition {name} completed successfully.\")\n",
        "        except Exception as e:\n",
        "            # If the pipeline fails, record the error.\n",
        "            error_msg = f\"Pipeline failed for GFC definition {name}: {e}\"\n",
        "            print(error_msg)\n",
        "            robustness_results['alternative_gfc_dummies'][name] = error_msg\n",
        "\n",
        "    # =========================================================================\n",
        "    # 3. Winsorized Data Analysis (Task 10, Step 3)\n",
        "    # =========================================================================\n",
        "    print(\"\\n--- Running Robustness Check 3: Winsorized Data ---\")\n",
        "    robustness_results['winsorized_data'] = {}\n",
        "    # Define the continuous variables to be winsorized.\n",
        "    vars_to_winsorize = [\n",
        "        's_it', 'b_it', 'a_it', 'y_tilde_it', 'g_tilde_it',\n",
        "        's_it_lag1', 'b_it_lag1'\n",
        "    ]\n",
        "    # Define the winsorization level (e.g., 1% from each tail).\n",
        "    winsorize_level = 0.01\n",
        "\n",
        "    print(f\"\\nRunning pipeline with data winsorized at {winsorize_level*100}%...\")\n",
        "    # Create a deep copy of the raw DataFrame.\n",
        "    df_copy = raw_df.copy(deep=True)\n",
        "\n",
        "    # Apply winsorization to each specified column.\n",
        "    for col in vars_to_winsorize:\n",
        "        # The winsorize function clips values below the 1st percentile and\n",
        "        # above the 99th percentile.\n",
        "        df_copy[col] = winsorize(\n",
        "            df_copy[col].dropna(), # Winsorize must operate on non-NaN data\n",
        "            limits=(winsorize_level, winsorize_level)\n",
        "        ).data\n",
        "\n",
        "    try:\n",
        "        # Re-run the entire analysis pipeline with the winsorized data.\n",
        "        results = run_fiscal_sustainability_analysis(df_copy, analysis_parameters)\n",
        "        # Store the final formatted table.\n",
        "        robustness_results['winsorized_data'][f'level_{winsorize_level:.2f}'.replace('.', '_')] = results['final_formatted_table']\n",
        "        print(f\"...Pipeline with winsorized data completed successfully.\")\n",
        "    except Exception as e:\n",
        "        # If the pipeline fails, record the error.\n",
        "        error_msg = f\"Pipeline failed for winsorized data: {e}\"\n",
        "        print(error_msg)\n",
        "        robustness_results['winsorized_data'][f'level_{winsorize_level:.2f}'.replace('.', '_')] = error_msg\n",
        "\n",
        "    return robustness_results\n"
      ],
      "metadata": {
        "id": "UkN_z9nbVW3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Sensitivity Analysis\n",
        "\n",
        "def run_sensitivity_analysis(\n",
        "    raw_df: pd.DataFrame,\n",
        "    analysis_parameters: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs a suite of sensitivity analyses on the main findings.\n",
        "\n",
        "    This function tests how the main results respond to changes in key data\n",
        "    definitions and underlying parameters. It leverages the main analysis\n",
        "    pipeline to ensure consistency across runs.\n",
        "\n",
        "    The analyses performed are:\n",
        "    1.  HP Filter Lambda Variation: Re-calculates cyclical components using\n",
        "        different smoothing parameters (λ) and re-runs the entire pipeline.\n",
        "    2.  Alternative High-Debt Classification: Re-defines the 'high-debt' and\n",
        "        'low-debt' country groups based on terciles instead of the median\n",
        "        and re-runs the pipeline.\n",
        "    3.  Outlier Country Exclusion: Systematically removes the highest-debt\n",
        "        countries one by one and re-runs the analysis to check for undue\n",
        "        influence.\n",
        "\n",
        "    Args:\n",
        "        raw_df (pd.DataFrame):\n",
        "            The raw input panel data. Crucially, for the HP filter analysis,\n",
        "            this DataFrame MUST contain the unfiltered series 'y_real' (real GDP)\n",
        "            and 'g_real' (real government spending).\n",
        "        analysis_parameters (Dict[str, Any]):\n",
        "            The baseline dictionary of analysis parameters.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A nested dictionary containing the final formatted results table\n",
        "            for each sensitivity analysis performed.\n",
        "    \"\"\"\n",
        "    # Initialize the master dictionary to store all sensitivity results.\n",
        "    sensitivity_results: Dict[str, Any] = {}\n",
        "\n",
        "    # =========================================================================\n",
        "    # 1. HP Filter Lambda Variation (Task 11, Step 1)\n",
        "    # =========================================================================\n",
        "    print(\"\\n--- Running Sensitivity Analysis 1: HP Filter Lambda Variation ---\")\n",
        "    sensitivity_results['hp_filter_sensitivity'] = {}\n",
        "    # Define the alternative lambda values to test.\n",
        "    alternative_lambdas = [6.25, 400]\n",
        "    # Define the required unfiltered source columns.\n",
        "    unfiltered_cols = ['y_real', 'g_real']\n",
        "\n",
        "    # Validate that the required unfiltered columns exist.\n",
        "    if not all(col in raw_df.columns for col in unfiltered_cols):\n",
        "        raise KeyError(\n",
        "            f\"Input 'raw_df' must contain {unfiltered_cols} for HP filter sensitivity analysis.\"\n",
        "        )\n",
        "\n",
        "    for lamb in alternative_lambdas:\n",
        "        print(f\"\\nRunning pipeline with HP filter lambda = {lamb}...\")\n",
        "        # Create a deep copy of the raw DataFrame for modification.\n",
        "        df_copy = raw_df.copy(deep=True)\n",
        "\n",
        "        # Define a helper function to apply the HP filter with the new lambda.\n",
        "        def apply_hp_filter(series: pd.Series) -> pd.Series:\n",
        "            # hpfilter returns (trend, cyclical_component)\n",
        "            _, cycle = hpfilter(series, lamb=lamb)\n",
        "            return cycle\n",
        "\n",
        "        # Re-calculate the cyclical components for y_tilde_it and g_tilde_it.\n",
        "        # Group by country to apply the filter to each time series individually.\n",
        "        df_copy['y_tilde_it'] = df_copy.groupby('country_iso')['y_real'].transform(apply_hp_filter)\n",
        "        df_copy['g_tilde_it'] = df_copy.groupby('country_iso')['g_real'].transform(apply_hp_filter)\n",
        "\n",
        "        try:\n",
        "            # Re-run the entire analysis pipeline with the re-calculated data.\n",
        "            results = run_fiscal_sustainability_analysis(df_copy, analysis_parameters)\n",
        "            # Store the final formatted table for this lambda value.\n",
        "            sensitivity_results['hp_filter_sensitivity'][f'lambda_{str(lamb).replace(\".\", \"_\")}'] = results['final_formatted_table']\n",
        "            print(f\"...Pipeline with lambda = {lamb} completed successfully.\")\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Pipeline failed for lambda = {lamb}: {e}\"\n",
        "            print(error_msg)\n",
        "            sensitivity_results['hp_filter_sensitivity'][f'lambda_{str(lamb).replace(\".\", \"_\")}'] = error_msg\n",
        "\n",
        "    # =========================================================================\n",
        "    # 2. Alternative High-Debt Classification (Task 11, Step 2)\n",
        "    # =========================================================================\n",
        "    print(\"\\n--- Running Sensitivity Analysis 2: Alternative High-Debt Definition ---\")\n",
        "    sensitivity_results['high_debt_definition_sensitivity'] = {}\n",
        "\n",
        "    print(\"\\nRunning pipeline with tercile-based debt groups...\")\n",
        "    # Create a deep copy of the parameters to modify the subsample definitions.\n",
        "    params_copy = copy.deepcopy(analysis_parameters)\n",
        "\n",
        "    # Calculate the median debt-to-GDP ratio for each country.\n",
        "    median_debt_by_country = raw_df.groupby('country_iso')['b_it'].median()\n",
        "\n",
        "    # Use pandas.qcut to divide countries into terciles (3 groups).\n",
        "    # labels=False returns integer indicators for the bins.\n",
        "    debt_terciles = pd.qcut(median_debt_by_country, q=3, labels=False)\n",
        "\n",
        "    # The lowest tercile (0) is the new 'low_debt' group.\n",
        "    new_low_debt = debt_terciles[debt_terciles == 0].index.tolist()\n",
        "    # The highest tercile (2) is the new 'high_debt' group.\n",
        "    new_high_debt = debt_terciles[debt_terciles == 2].index.tolist()\n",
        "\n",
        "    # Overwrite the subsample definitions in the copied parameters.\n",
        "    params_copy['subsample_definitions']['low_debt'] = new_low_debt\n",
        "    params_copy['subsample_definitions']['high_debt'] = new_high_debt\n",
        "\n",
        "    try:\n",
        "        # Re-run the entire analysis pipeline with the new country groups.\n",
        "        results = run_fiscal_sustainability_analysis(raw_df, params_copy)\n",
        "        # Store the final formatted table.\n",
        "        sensitivity_results['high_debt_definition_sensitivity']['terciles'] = results['final_formatted_table']\n",
        "        print(\"...Pipeline with tercile-based debt groups completed successfully.\")\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Pipeline failed for tercile-based debt groups: {e}\"\n",
        "        print(error_msg)\n",
        "        sensitivity_results['high_debt_definition_sensitivity']['terciles'] = error_msg\n",
        "\n",
        "    # =========================================================================\n",
        "    # 3. Outlier Country Exclusion (Task 11, Step 3)\n",
        "    # =========================================================================\n",
        "    print(\"\\n--- Running Sensitivity Analysis 3: Outlier Country Exclusion ---\")\n",
        "    sensitivity_results['outlier_country_exclusion'] = {}\n",
        "\n",
        "    # Identify the top 3 highest-debt countries based on median debt.\n",
        "    # This is a more focused check than excluding the top 5%.\n",
        "    if 'median_debt_by_country' not in locals(): # Recalculate if not available\n",
        "        median_debt_by_country = raw_df.groupby('country_iso')['b_it'].median()\n",
        "\n",
        "    # Get the ISO codes of the top 3 countries with the highest median debt.\n",
        "    outlier_countries = median_debt_by_country.nlargest(3).index.tolist()\n",
        "\n",
        "    # Loop through each identified outlier country.\n",
        "    for country_to_exclude in outlier_countries:\n",
        "        print(f\"\\nRunning pipeline excluding country: {country_to_exclude}...\")\n",
        "        # Create a copy of the raw DataFrame that excludes the outlier country.\n",
        "        df_copy = raw_df[raw_df['country_iso'] != country_to_exclude].copy(deep=True)\n",
        "\n",
        "        try:\n",
        "            # Re-run the entire analysis pipeline on the N-1 panel.\n",
        "            results = run_fiscal_sustainability_analysis(df_copy, analysis_parameters)\n",
        "            # Store the final formatted table.\n",
        "            sensitivity_results['outlier_country_exclusion'][f'exclude_{country_to_exclude}'] = results['final_formatted_table']\n",
        "            print(f\"...Pipeline excluding {country_to_exclude} completed successfully.\")\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Pipeline failed excluding {country_to_exclude}: {e}\"\n",
        "            print(error_msg)\n",
        "            sensitivity_results['outlier_country_exclusion'][f'exclude_{country_to_exclude}'] = error_msg\n",
        "\n",
        "    return sensitivity_results\n"
      ],
      "metadata": {
        "id": "COE94xBLVzpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Interpretation and Discussion\n",
        "\n",
        "def generate_results_interpretation(\n",
        "    estimation_results: Dict[str, Dict[str, Any]]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates a structured, text-based interpretation of the model results.\n",
        "\n",
        "    This function synthesizes the quantitative outputs from the estimation\n",
        "    pipeline into a qualitative and economic discussion, mirroring the analysis\n",
        "    section of a research paper. It focuses on three key areas:\n",
        "    1.  Cross-Model Comparison: Compares the key debt response coefficient (ρ)\n",
        "        and long-run response (LRR) across different country subsamples.\n",
        "    2.  Economic Significance: Translates the estimated coefficients into\n",
        "        policy-relevant magnitudes (e.g., the fiscal adjustment following a\n",
        "        10-point increase in the debt-to-GDP ratio).\n",
        "    3.  Fiscal Sustainability Assessment: Provides a summary judgment on fiscal\n",
        "        sustainability for each group based on the sign and statistical\n",
        "        significance of the fiscal response.\n",
        "\n",
        "    Args:\n",
        "        estimation_results (Dict[str, Dict[str, Any]]):\n",
        "            The fully populated dictionary of results from the analysis pipeline.\n",
        "\n",
        "    Returns:\n",
        "        str:\n",
        "            A formatted markdown string containing the detailed interpretation\n",
        "            of the results.\n",
        "    \"\"\"\n",
        "    # =========================================================================\n",
        "    # 0. Initial Setup and Helper Function\n",
        "    # =========================================================================\n",
        "    # Initialize a list to hold the paragraphs of the markdown report.\n",
        "    report_parts = [\"# Interpretation of Econometric Results\\n\"]\n",
        "\n",
        "    def get_res(model: str, metric: str, key: str, default: Any = np.nan) -> Any:\n",
        "        \"\"\"Safely retrieves a nested result, returning a default if not found.\"\"\"\n",
        "        return estimation_results.get(model, {}).get(metric, {}).get(key, default)\n",
        "\n",
        "    # =========================================================================\n",
        "    # 1. Overall Finding from the Aggregate Full Model (col_2)\n",
        "    # =========================================================================\n",
        "    report_parts.append(\"## 1. Aggregate Panel Findings (Full Model)\\n\")\n",
        "\n",
        "    # Extract key results for the main model (column 2).\n",
        "    rho_agg = get_res('col_2', 'coefficients', 'b_it_lag1')\n",
        "    lrr_agg = get_res('col_2', 'long_run_effects', 'point_estimate')\n",
        "    lrr_agg_pval = get_res('col_2', 'long_run_effects', 'p_value')\n",
        "\n",
        "    # Calculate the economic significance.\n",
        "    adjustment_for_10_pct = lrr_agg * 10 if pd.notna(lrr_agg) else \"N/A\"\n",
        "\n",
        "    # Generate the paragraph.\n",
        "    p1 = (\n",
        "        \"The analysis of the full aggregate panel (52 countries) reveals a \"\n",
        "        \"statistically significant fiscal reaction to public debt accumulation. \"\n",
        "        f\"The estimated short-run response of the primary surplus to a one \"\n",
        "        f\"percentage point increase in the lagged debt-to-GDP ratio (ρ) is **{rho_agg:.3f}**.\"\n",
        "    )\n",
        "    report_parts.append(p1)\n",
        "\n",
        "    p2 = (\n",
        "        f\"More importantly, the long-run response (LRR), which accounts for policy inertia, \"\n",
        "        f\"is estimated to be **{lrr_agg:.3f}**. This result is \"\n",
        "        f\"{'statistically significant' if lrr_agg_pval < 0.1 else 'not statistically significant'} \"\n",
        "        f\"(p-value: {lrr_agg_pval:.3f}).\"\n",
        "    )\n",
        "    report_parts.append(p2)\n",
        "\n",
        "    p3 = (\n",
        "        f\"**Economic Significance**: This implies that, on average, a **10 percentage point** \"\n",
        "        f\"increase in the debt-to-GDP ratio prompts a long-run fiscal adjustment that \"\n",
        "        f\"raises the primary surplus-to-GDP ratio by **{adjustment_for_10_pct:.2f} percentage points**. \"\n",
        "        f\"This indicates a clear tendency towards mean-reverting debt dynamics, a cornerstone of \"\n",
        "        f\"fiscal sustainability.\"\n",
        "    )\n",
        "    report_parts.append(p3)\n",
        "\n",
        "    # =========================================================================\n",
        "    # 2. Cross-Model Comparison: High-Debt vs. Low-Debt (Task 12, Step 1)\n",
        "    # =========================================================================\n",
        "    report_parts.append(\"\\n## 2. Heterogeneity by Debt Level\\n\")\n",
        "\n",
        "    # Extract LRR for high-debt (col_4) and low-debt (col_6) groups.\n",
        "    lrr_high = get_res('col_4', 'long_run_effects', 'point_estimate')\n",
        "    lrr_low = get_res('col_6', 'long_run_effects', 'point_estimate')\n",
        "\n",
        "    # Calculate the percentage difference.\n",
        "    if pd.notna(lrr_high) and pd.notna(lrr_low) and lrr_low != 0:\n",
        "        diff_pct_debt = ((lrr_low - lrr_high) / lrr_low) * 100\n",
        "        comparison_text = f\"The response in the high-debt group is **{diff_pct_debt:.1f}% lower** than in the low-debt group.\"\n",
        "    else:\n",
        "        comparison_text = \"A direct percentage comparison is not available.\"\n",
        "\n",
        "    p4 = (\n",
        "        \"The analysis reveals significant heterogeneity based on countries' debt levels:\\n\"\n",
        "        f\"- **Low-Debt Countries (col 6)**: Exhibit a strong and significant long-run fiscal response of **{lrr_low:.3f}**.\\n\"\n",
        "        f\"- **High-Debt Countries (col 4)**: Also show a statistically significant positive response, but it is more muted at **{lrr_high:.3f}**.\\n\"\n",
        "        f\"- **Comparison**: {comparison_text} This suggests that while high-debt countries still react \"\n",
        "        \"correctively, their capacity or willingness for fiscal adjustment may be constrained.\"\n",
        "    )\n",
        "    report_parts.append(p4)\n",
        "\n",
        "    # =========================================================================\n",
        "    # 3. Cross-Model Comparison: Industrial vs. Emerging (Task 12, Step 1)\n",
        "    # =========================================================================\n",
        "    report_parts.append(\"\\n## 3. Heterogeneity by Economic Status\\n\")\n",
        "\n",
        "    # Extract LRR for industrial (col_8) and emerging (col_10) groups.\n",
        "    lrr_ind = get_res('col_8', 'long_run_effects', 'point_estimate')\n",
        "    lrr_emg = get_res('col_10', 'long_run_effects', 'point_estimate')\n",
        "\n",
        "    # Calculate the percentage difference.\n",
        "    if pd.notna(lrr_ind) and pd.notna(lrr_emg) and lrr_ind != 0:\n",
        "        diff_pct_econ = ((lrr_ind - lrr_emg) / lrr_ind) * 100\n",
        "        comparison_text_econ = f\"The response in emerging economies is **{diff_pct_econ:.1f}% lower** than in industrial countries.\"\n",
        "    else:\n",
        "        comparison_text_econ = \"A direct percentage comparison is not available.\"\n",
        "\n",
        "    p5 = (\n",
        "        \"A pronounced difference in fiscal conduct is observed between industrial and emerging economies:\\n\"\n",
        "        f\"- **Industrial Countries (col 8)**: Demonstrate a robust long-run response of **{lrr_ind:.3f}**.\\n\"\n",
        "        f\"- **Emerging Economies (col 10)**: Show a positive but considerably weaker response of **{lrr_emg:.3f}**.\\n\"\n",
        "        f\"- **Comparison**: {comparison_text_econ} This highlights the stronger institutional frameworks and \"\n",
        "        \"deeper markets that likely enable more credible fiscal adjustments in industrial nations.\"\n",
        "    )\n",
        "    report_parts.append(p5)\n",
        "\n",
        "    # =========================================================================\n",
        "    # 4. Fiscal Sustainability Assessment (Task 12, Step 3)\n",
        "    # =========================================================================\n",
        "    report_parts.append(\"\\n## 4. Overall Fiscal Sustainability Assessment\\n\")\n",
        "\n",
        "    # Define a helper to generate the assessment string.\n",
        "    def assess_sustainability(lrr, p_val):\n",
        "        if pd.isna(lrr) or pd.isna(p_val):\n",
        "            return \"Inconclusive due to missing data.\"\n",
        "        if lrr > 0 and p_val < 0.1:\n",
        "            return \"Consistent with a sustainable, Ricardian fiscal policy.\"\n",
        "        elif lrr > 0 and p_val >= 0.1:\n",
        "            return \"Shows a positive but not statistically significant response, suggesting weak or uncertain sustainability.\"\n",
        "        else:\n",
        "            return \"Not consistent with a sustainable fiscal policy (response is non-positive or insignificant).\"\n",
        "\n",
        "    # Generate assessments for each key group.\n",
        "    assessment_agg = assess_sustainability(lrr_agg, lrr_agg_pval)\n",
        "    assessment_high = assess_sustainability(lrr_high, get_res('col_4', 'long_run_effects', 'p_value'))\n",
        "    assessment_low = assess_sustainability(lrr_low, get_res('col_6', 'long_run_effects', 'p_value'))\n",
        "    assessment_ind = assess_sustainability(lrr_ind, get_res('col_8', 'long_run_effects', 'p_value'))\n",
        "    assessment_emg = assess_sustainability(lrr_emg, get_res('col_10', 'long_run_effects', 'p_value'))\n",
        "\n",
        "    p6 = (\n",
        "        \"Based on the long-run response to debt, we can summarize the fiscal stance of the different groups:\\n\"\n",
        "        f\"- **Aggregate Panel**: {assessment_agg}\\n\"\n",
        "        f\"- **High-Debt Countries**: {assessment_high}\\n\"\n",
        "        f\"- **Low-Debt Countries**: {assessment_low}\\n\"\n",
        "        f\"- **Industrial Countries**: {assessment_ind}\\n\"\n",
        "        f\"- **Emerging Economies**: {assessment_emg}\\n\\n\"\n",
        "        \"The overall conclusion is that a majority of governments across the sample do react to rising public \"\n",
        "        \"debt in a manner that avoids explosive debt paths. However, the strength and statistical certainty of \"\n",
        "        \"this reaction varies significantly across country groups.\"\n",
        "    )\n",
        "    report_parts.append(p6)\n",
        "\n",
        "    # Join all parts into a single markdown string.\n",
        "    return \"\\n\".join(report_parts)\n"
      ],
      "metadata": {
        "id": "qJ1WvKBvX1vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Visualization\n",
        "\n",
        "def _run_rolling_window_estimation(\n",
        "    panel_df: pd.DataFrame,\n",
        "    analysis_parameters: Dict[str, Any],\n",
        "    window_size: int = 10\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Helper to run DCCEMG estimation on a rolling window of the data.\"\"\"\n",
        "    # This is a computationally intensive helper function.\n",
        "    all_years = sorted(panel_df.index.get_level_values('year').unique())\n",
        "    rolling_results = []\n",
        "\n",
        "    # Loop through the data with a rolling window.\n",
        "    for i in range(len(all_years) - window_size + 1):\n",
        "        start_year = all_years[i]\n",
        "        end_year = all_years[i + window_size - 1]\n",
        "\n",
        "        # Slice the DataFrame for the current window.\n",
        "        window_df = panel_df.loc[(slice(None), slice(start_year, end_year)), :]\n",
        "\n",
        "        # For simplicity, we run only the main aggregate model (col_2).\n",
        "        spec = analysis_parameters['model_specifications']['col_2']\n",
        "\n",
        "        # Run the estimation on the windowed data.\n",
        "        # Note: This re-uses the core estimation logic.\n",
        "        result = _estimate_single_dccemg_model(\n",
        "            window_df,\n",
        "            's_it',\n",
        "            spec['regressors'],\n",
        "            analysis_parameters['dcce_lags']['value']\n",
        "        )\n",
        "\n",
        "        # Store the key coefficient and the end year of the window.\n",
        "        if 'coefficients' in result and 'b_it_lag1' in result['coefficients']:\n",
        "            rolling_results.append({\n",
        "                'year': end_year,\n",
        "                'rho_estimate': result['coefficients']['b_it_lag1']\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(rolling_results).set_index('year')\n",
        "\n",
        "\n",
        "def generate_visualizations(\n",
        "    estimation_results: Dict[str, Dict[str, Any]],\n",
        "    analysis_dataframe: pd.DataFrame,\n",
        "    analysis_parameters: Dict[str, Any],\n",
        "    enable_rolling_estimation: bool = False\n",
        ") -> Dict[str, Figure]:\n",
        "    \"\"\"\n",
        "    Generates a set of publication-quality visualizations of the results.\n",
        "\n",
        "    This function creates three key plots to visually summarize the findings:\n",
        "    1.  Coefficient Forest Plot: Compares the crucial debt response coefficient\n",
        "        (ρ) and its 95% confidence interval across key model specifications.\n",
        "    2.  Faceted Scatter Plot: Shows the relationship between the primary\n",
        "        surplus and lagged debt, conditioned on country group characteristics.\n",
        "    3.  Time Series of Coefficients: Plots the evolution of the debt response\n",
        "        coefficient over time using a rolling window estimation (optional and\n",
        "        computationally intensive).\n",
        "\n",
        "    Args:\n",
        "        estimation_results (Dict[str, Dict[str, Any]]):\n",
        "            The fully populated dictionary of results from the analysis pipeline.\n",
        "        analysis_dataframe (pd.DataFrame):\n",
        "            The final, cleansed, and structured DataFrame used for analysis.\n",
        "        analysis_parameters (Dict[str, Any]):\n",
        "            The dictionary of analysis parameters.\n",
        "        enable_rolling_estimation (bool):\n",
        "            If True, the computationally intensive rolling window estimation\n",
        "            for the time-series plot will be performed. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Figure]:\n",
        "            A dictionary where keys are plot names and values are the\n",
        "            matplotlib Figure objects.\n",
        "    \"\"\"\n",
        "    # Set a professional plotting style.\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    # Initialize the dictionary to store the figure objects.\n",
        "    figures: Dict[str, Figure] = {}\n",
        "\n",
        "    # =========================================================================\n",
        "    # 1. Coefficient Forest Plot (Task 13, Step 1)\n",
        "    # =========================================================================\n",
        "    # Select key models for comparison.\n",
        "    models_to_plot = {\n",
        "        'Aggregate': 'col_2',\n",
        "        'High-Debt': 'col_4',\n",
        "        'Low-Debt': 'col_6',\n",
        "        'Industrial': 'col_8',\n",
        "        'Emerging': 'col_10'\n",
        "    }\n",
        "    plot_data = []\n",
        "    for name, model_key in models_to_plot.items():\n",
        "        res = estimation_results.get(model_key, {})\n",
        "        coeff = res.get('coefficients', {}).get('b_it_lag1', np.nan)\n",
        "        se = res.get('std_errors', {}).get('b_it_lag1', np.nan)\n",
        "        plot_data.append({'model': name, 'coeff': coeff, 'se': se})\n",
        "\n",
        "    plot_df = pd.DataFrame(plot_data).set_index('model')\n",
        "    # Calculate 95% confidence interval error margin (z-score ~1.96).\n",
        "    plot_df['error'] = 1.96 * plot_df['se']\n",
        "\n",
        "    # Create the figure and axes.\n",
        "    fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
        "    # Plot the point estimates and error bars.\n",
        "    ax1.errorbar(\n",
        "        x=plot_df['coeff'], y=plot_df.index, xerr=plot_df['error'],\n",
        "        fmt='o', capsize=5, color='darkblue', ecolor='gray', elinewidth=1\n",
        "    )\n",
        "    # Add a vertical line at zero for easy significance assessment.\n",
        "    ax1.axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
        "    # Set titles and labels.\n",
        "    ax1.set_title('Fiscal Response to Debt (ρ) Across Subsamples', fontsize=16)\n",
        "    ax1.set_xlabel('Coefficient Estimate for Lagged Debt-to-GDP Ratio')\n",
        "    ax1.set_ylabel('Country Group')\n",
        "    ax1.invert_yaxis() # Display models from top to bottom\n",
        "    fig1.tight_layout()\n",
        "    figures['coefficient_forest_plot'] = fig1\n",
        "\n",
        "    # =========================================================================\n",
        "    # 2. Faceted Scatter Plot (Task 13, Step 2)\n",
        "    # =========================================================================\n",
        "    # Prepare data for seaborn's lmplot.\n",
        "    scatter_df = analysis_dataframe.copy()\n",
        "    scatter_df['Debt Group'] = np.where(scatter_df['high_debt_indicator'] == 1, 'High-Debt', 'Low-Debt')\n",
        "    scatter_df['Economic Status'] = np.where(scatter_df['industrial_indicator'] == 1, 'Industrial', 'Emerging')\n",
        "\n",
        "    # Create the faceted plot.\n",
        "    # `lmplot` is a figure-level function, so it returns a FacetGrid object.\n",
        "    g = sns.lmplot(\n",
        "        data=scatter_df,\n",
        "        x='b_it_lag1',\n",
        "        y='s_it',\n",
        "        col='Debt Group',\n",
        "        hue='Economic Status',\n",
        "        height=5,\n",
        "        aspect=1.2,\n",
        "        scatter_kws={'alpha': 0.3},\n",
        "        line_kws={'linewidth': 2}\n",
        "    )\n",
        "    # Set titles and labels for the FacetGrid.\n",
        "    g.fig.suptitle('Primary Surplus vs. Lagged Debt', y=1.03, fontsize=16)\n",
        "    g.set_axis_labels('Lagged Debt-to-GDP Ratio', 'Primary Surplus-to-GDP Ratio')\n",
        "    figures['faceted_scatter_plot'] = g.fig\n",
        "\n",
        "    # =========================================================================\n",
        "    # 3. Time Series of Coefficients (Task 13, Step 3)\n",
        "    # =========================================================================\n",
        "    if enable_rolling_estimation:\n",
        "        print(\"Performing computationally intensive rolling window estimation...\")\n",
        "        # Run the rolling window estimation.\n",
        "        rolling_coeffs = _run_rolling_window_estimation(\n",
        "            analysis_dataframe, analysis_parameters, window_size=10\n",
        "        )\n",
        "\n",
        "        # Create the figure and axes.\n",
        "        fig3, ax3 = plt.subplots(figsize=(12, 6))\n",
        "        # Plot the time series of the estimated coefficient.\n",
        "        ax3.plot(rolling_coeffs.index, rolling_coeffs['rho_estimate'], marker='o', linestyle='-', color='darkgreen')\n",
        "        # Add a horizontal line at zero.\n",
        "        ax3.axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
        "        # Set titles and labels.\n",
        "        ax3.set_title('Time-Varying Fiscal Response to Debt (ρ) - 10-Year Rolling Window', fontsize=16)\n",
        "        ax3.set_xlabel('Year (End of Window)')\n",
        "        ax3.set_ylabel('Estimated Coefficient (ρ)')\n",
        "        fig3.tight_layout()\n",
        "        figures['rolling_coefficient_plot'] = fig3\n",
        "        print(\"...Rolling window estimation and plot complete.\")\n",
        "    else:\n",
        "        figures['rolling_coefficient_plot'] = \"Plot not generated. Set enable_rolling_estimation=True.\"\n",
        "\n",
        "    # Close all plots to prevent them from displaying in-line automatically.\n",
        "    plt.close('all')\n",
        "\n",
        "    return figures\n"
      ],
      "metadata": {
        "id": "tSJZPJQwaf-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Master Pipeline Function\n",
        "\n",
        "def create_fiscal_credibility_report(\n",
        "    raw_df: pd.DataFrame,\n",
        "    analysis_parameters: Dict[str, Any],\n",
        "    enable_intensive_visuals: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete, end-to-end fiscal credibility analysis and compiles a master report.\n",
        "\n",
        "    This grand orchestrator function serves as the single entry point to run the\n",
        "    entire research project. It sequentially executes the baseline analysis,\n",
        "    a series of robustness and sensitivity checks, and finally generates\n",
        "    interpretive text and visualizations.\n",
        "\n",
        "    The final output is a comprehensive dictionary containing every artifact\n",
        "    generated during the analysis, providing a full, auditable record.\n",
        "\n",
        "    Args:\n",
        "        raw_df (pd.DataFrame):\n",
        "            The raw input panel data. Must contain all required columns for all\n",
        "            analyses, including unfiltered series for sensitivity checks\n",
        "            (e.g., 'y_real', 'g_real').\n",
        "        analysis_parameters (Dict[str, Any]):\n",
        "            The baseline dictionary of analysis parameters.\n",
        "        enable_intensive_visuals (bool):\n",
        "            Flag to enable computationally expensive visualizations, such as\n",
        "            the rolling-window coefficient plot. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            The master results dictionary containing all reports and artifacts\n",
        "            from the entire analytical workflow.\n",
        "    \"\"\"\n",
        "    # Initialize the final, all-encompassing results dictionary.\n",
        "    master_report: Dict[str, Any] = {}\n",
        "    print(\"=\"*80)\n",
        "    print(\"STARTING END-TO-END FISCAL CREDIBILITY ANALYSIS PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # =========================================================================\n",
        "    # STAGE I: BASELINE ANALYSIS (Tasks 1-9)\n",
        "    # =========================================================================\n",
        "    try:\n",
        "        print(\"\\n>>> STAGE I: EXECUTING BASELINE ANALYSIS...\\n\")\n",
        "        # Run the main end-to-end pipeline to get the baseline results.\n",
        "        baseline_results = run_fiscal_sustainability_analysis(raw_df, analysis_parameters)\n",
        "        # Store the entire output of the baseline run.\n",
        "        master_report['baseline_analysis'] = baseline_results\n",
        "        print(\"\\n>>> STAGE I: BASELINE ANALYSIS COMPLETED SUCCESSFULLY.\")\n",
        "    except Exception as e:\n",
        "        # If the baseline fails, the entire process must stop.\n",
        "        print(f\"\\nFATAL ERROR: Baseline analysis failed. Halting execution.\")\n",
        "        print(f\"Error details: {e}\")\n",
        "        # Add the traceback for detailed debugging.\n",
        "        master_report['error_log'] = traceback.format_exc()\n",
        "        return master_report\n",
        "\n",
        "    # =========================================================================\n",
        "    # STAGE II: ROBUSTNESS CHECKS (Task 10)\n",
        "    # =========================================================================\n",
        "    try:\n",
        "        print(\"\\n>>> STAGE II: EXECUTING ROBUSTNESS CHECKS...\\n\")\n",
        "        # Run the suite of robustness checks.\n",
        "        robustness_results = run_robustness_checks(raw_df, analysis_parameters)\n",
        "        # Store the results of the robustness checks.\n",
        "        master_report['robustness_checks'] = robustness_results\n",
        "        print(\"\\n>>> STAGE II: ROBUSTNESS CHECKS COMPLETED SUCCESSFULLY.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Robustness checks failed to complete.\")\n",
        "        print(f\"Error details: {e}\")\n",
        "        # Store the error but allow the pipeline to continue to other stages.\n",
        "        master_report['robustness_checks'] = {'error': traceback.format_exc()}\n",
        "\n",
        "    # =========================================================================\n",
        "    # STAGE III: SENSITIVITY ANALYSIS (Task 11)\n",
        "    # =========================================================================\n",
        "    try:\n",
        "        print(\"\\n>>> STAGE III: EXECUTING SENSITIVITY ANALYSIS...\\n\")\n",
        "        # Run the suite of sensitivity analyses.\n",
        "        sensitivity_results = run_sensitivity_analysis(raw_df, analysis_parameters)\n",
        "        # Store the results of the sensitivity analyses.\n",
        "        master_report['sensitivity_analysis'] = sensitivity_results\n",
        "        print(\"\\n>>> STAGE III: SENSITIVITY ANALYSIS COMPLETED SUCCESSFULLY.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Sensitivity analysis failed to complete.\")\n",
        "        print(f\"Error details: {e}\")\n",
        "        master_report['sensitivity_analysis'] = {'error': traceback.format_exc()}\n",
        "\n",
        "    # =========================================================================\n",
        "    # STAGE IV: INTERPRETATION AND VISUALIZATION (Tasks 12 & 13)\n",
        "    # =========================================================================\n",
        "    # These stages operate on the successful baseline results.\n",
        "\n",
        "    # --- Textual Interpretation (Task 12) ---\n",
        "    try:\n",
        "        print(\"\\n>>> STAGE IVa: GENERATING TEXTUAL INTERPRETATION...\\n\")\n",
        "        # Generate the markdown report based on the baseline estimation results.\n",
        "        interpretation_text = generate_results_interpretation(\n",
        "            master_report['baseline_analysis']['estimation_results']\n",
        "        )\n",
        "        master_report['textual_interpretation'] = interpretation_text\n",
        "        print(\"\\n>>> STAGE IVa: TEXTUAL INTERPRETATION GENERATED SUCCESSFULLY.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Failed to generate textual interpretation.\")\n",
        "        print(f\"Error details: {e}\")\n",
        "        master_report['textual_interpretation'] = {'error': traceback.format_exc()}\n",
        "\n",
        "    # --- Visualization (Task 13) ---\n",
        "    try:\n",
        "        print(\"\\n>>> STAGE IVb: GENERATING VISUALIZATIONS...\\n\")\n",
        "        # Generate the dictionary of plot figures.\n",
        "        visualizations = generate_visualizations(\n",
        "            master_report['baseline_analysis']['estimation_results'],\n",
        "            master_report['baseline_analysis']['analysis_dataframe'],\n",
        "            analysis_parameters,\n",
        "            enable_intensive_visuals\n",
        "        )\n",
        "        master_report['visualizations'] = visualizations\n",
        "        print(\"\\n>>> STAGE IVb: VISUALIZATIONS GENERATED SUCCESSFULLY.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Failed to generate visualizations.\")\n",
        "        print(f\"Error details: {e}\")\n",
        "        master_report['visualizations'] = {'error': traceback.format_exc()}\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FISCAL CREDIBILITY ANALYSIS PIPELINE FINISHED\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Return the final, comprehensive master report dictionary.\n",
        "    return master_report"
      ],
      "metadata": {
        "id": "M1gy2JZMdKWC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}